{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25b9438",
   "metadata": {},
   "source": [
    "# Elasticsearch Embedding Pipeline Simulation\n",
    "\n",
    "Notebook ini mensimulasikan pipeline embedding Elasticsearch berdasarkan kode dari `benchmarks/embedding/contrib/pipeline/pipeline.py`. \n",
    "\n",
    "Pipeline ini melakukan:\n",
    "1. **Data Loading**: Membaca data dari format JSONL\n",
    "2. **Text Processing**: Tokenisasi dan truncation berdasarkan token limit\n",
    "3. **Vector Indexing**: Batch indexing ke Elasticsearch dengan embeddings\n",
    "4. **Semantic Retrieval**: Pencarian semantik dengan similarity vector\n",
    "5. **Optional Reranking**: Reranking hasil retrieval untuk akurasi yang lebih baik\n",
    "\n",
    "## Overview Pipeline Architecture\n",
    "\n",
    "```\n",
    "JSONL Data ‚Üí Text Processing ‚Üí Document Creation ‚Üí Chunk Conversion ‚Üí \n",
    "Elasticsearch Index (Vector Embeddings + Metadata) ‚Üí Vector Retrieval ‚Üí \n",
    "Optional Reranking ‚Üí Final Results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab1c44",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import semua library yang diperlukan untuk simulasi pipeline Elasticsearch embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0e357e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Timestamp: 2025-07-24 08:29:47.501789\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Elasticsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk, parallel_bulk\n",
    "\n",
    "# Transformers & Tokenization  \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Langchain components (simulated classes)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Utility libraries\n",
    "import uuid\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e5d33",
   "metadata": {},
   "source": [
    "## 2. Define Data Structures and Configuration Classes\n",
    "\n",
    "Mendefinisikan struktur data yang digunakan dalam pipeline embedding, termasuk konfigurasi model dan document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c665a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data structures defined successfully!\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration similar to the original pipeline.\"\"\"\n",
    "    provider: str = \"huggingface\"\n",
    "    provider_model_id: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_dimensions: int = 384\n",
    "    max_tokens: int = 512\n",
    "    \n",
    "    def model_dump(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Document class similar to langchain Document.\"\"\"\n",
    "    page_content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass \n",
    "class Chunk:\n",
    "    \"\"\"Chunk class from the original pipeline.\"\"\"\n",
    "    content: str\n",
    "    id: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingState:\n",
    "    \"\"\"State management for pipeline.\"\"\"\n",
    "    id: str\n",
    "    query: str\n",
    "    dataset: str\n",
    "    retrieved_chunks: List[Chunk] = None\n",
    "    reranked_chunks: List[Chunk] = None\n",
    "    supporting_facts: List[str] = None\n",
    "\n",
    "class EmbeddingStateKeys:\n",
    "    \"\"\"State keys constants.\"\"\"\n",
    "    ID = \"id\"\n",
    "    QUERY = \"query\"\n",
    "    DATASET = \"dataset\"\n",
    "    RETRIEVED_CHUNKS = \"retrieved_chunks\"\n",
    "    RERANKED_CHUNKS = \"reranked_chunks\"\n",
    "    SUPPORTING_FACTS = \"supporting_facts\"\n",
    "\n",
    "print(\"‚úÖ Data structures defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06282523",
   "metadata": {},
   "source": [
    "## 3. Setup Elasticsearch Connection\n",
    "\n",
    "Konfigurasi dan koneksi ke Elasticsearch. Untuk simulasi, kita akan menggunakan koneksi lokal atau Docker Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bfe2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Elasticsearch configuration ready!\n",
      "üìù Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "üîç Embedding dimensions: 384\n"
     ]
    }
   ],
   "source": [
    "class ElasticsearchVectorStore:\n",
    "    \"\"\"Simulated ElasticsearchVectorDataStore class.\"\"\"\n",
    "    \n",
    "    def __init__(self, index_name: str, embedding_config: Dict[str, Any]):\n",
    "        self.index_name = index_name\n",
    "        self.embedding_config = embedding_config\n",
    "        \n",
    "        # Elasticsearch connection (sesuaikan dengan setup lokal)\n",
    "        self.client = Elasticsearch([\n",
    "            {'host': 'localhost', 'port': 9200, 'scheme': 'http'}\n",
    "        ])\n",
    "        \n",
    "        # Untuk simulasi, kita akan menggunakan in-memory storage jika ES tidak tersedia\n",
    "        self.use_simulation = False\n",
    "        self.simulated_docs = []\n",
    "        \n",
    "        try:\n",
    "            # Test connection\n",
    "            info = self.client.info()\n",
    "            logger.info(f\"‚úÖ Connected to Elasticsearch: {info['version']['number']}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Cannot connect to Elasticsearch: {e}\")\n",
    "            logger.info(\"üîÑ Using in-memory simulation mode\")\n",
    "            self.use_simulation = True\n",
    "            \n",
    "        self._create_index_if_not_exists()\n",
    "    \n",
    "    def _create_index_if_not_exists(self):\n",
    "        \"\"\"Create index with proper mapping for vector search.\"\"\"\n",
    "        if self.use_simulation:\n",
    "            return\n",
    "            \n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": self.embedding_config.get(\"embedding_dimensions\", 384)\n",
    "                    },\n",
    "                    \"metadata\": {\"type\": \"object\"},\n",
    "                    \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                    \"timestamp\": {\"type\": \"date\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if not self.client.indices.exists(index=self.index_name):\n",
    "                self.client.indices.create(index=self.index_name, body=mapping)\n",
    "                logger.info(f\"‚úÖ Created index: {self.index_name}\")\n",
    "            else:\n",
    "                logger.info(f\"üìã Index already exists: {self.index_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error creating index: {e}\")\n",
    "    \n",
    "    def count_documents(self) -> int:\n",
    "        \"\"\"Count documents in index.\"\"\"\n",
    "        if self.use_simulation:\n",
    "            return len(self.simulated_docs)\n",
    "        \n",
    "        try:\n",
    "            result = self.client.count(index=self.index_name)\n",
    "            return result['count']\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def add_chunks_batch(self, chunks: List[Chunk], embeddings: List[List[float]]):\n",
    "        \"\"\"Add chunks with embeddings in batch.\"\"\"\n",
    "        if self.use_simulation:\n",
    "            for chunk, embedding in zip(chunks, embeddings):\n",
    "                self.simulated_docs.append({\n",
    "                    'chunk': chunk,\n",
    "                    'embedding': embedding\n",
    "                })\n",
    "            return\n",
    "        \n",
    "        # Prepare documents for bulk indexing\n",
    "        docs = []\n",
    "        for chunk, embedding in zip(chunks, embeddings):\n",
    "            doc = {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_id\": chunk.id,\n",
    "                \"_source\": {\n",
    "                    \"content\": chunk.content,\n",
    "                    \"embedding\": embedding,\n",
    "                    \"metadata\": chunk.metadata,\n",
    "                    \"chunk_id\": chunk.id,\n",
    "                    \"timestamp\": datetime.now()\n",
    "                }\n",
    "            }\n",
    "            docs.append(doc)\n",
    "        \n",
    "        # Bulk index\n",
    "        try:\n",
    "            success, failed = bulk(self.client, docs)\n",
    "            logger.info(f\"‚úÖ Indexed {success} documents, {len(failed)} failed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Bulk indexing error: {e}\")\n",
    "\n",
    "# Initialize configuration\n",
    "model_config = ModelConfig()\n",
    "pipeline_config = {\n",
    "    \"vector_store_provider\": \"elasticsearch\",\n",
    "    \"chunks_file_name\": \"corpus.jsonl\",\n",
    "    \"retrieval_top_k\": 10,\n",
    "    \"truncate_chunk_size\": 512,\n",
    "    \"use_reranker\": False,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Elasticsearch configuration ready!\")\n",
    "print(f\"üìù Model: {model_config.provider_model_id}\")\n",
    "print(f\"üîç Embedding dimensions: {model_config.embedding_dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de11c9",
   "metadata": {},
   "source": [
    "## 4. Create Mock Data Structure\n",
    "\n",
    "Generate sample JSONL data similar to format corpus.jsonl untuk testing pipeline. Data ini mensimulasikan dokumen yang akan diindex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa7409d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 10 mock documents\n",
      "\n",
      "üìÑ Sample document:\n",
      "{\n",
      "  \"_id\": \"sample_dataset_chunk_0000\",\n",
      "  \"text\": \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
      "  \"title\": \"Document 1\",\n",
      "  \"source\": \"sample_source_1\",\n",
      "  \"category\": \"technology\",\n",
      "  \"chunk_index\": 0,\n",
      "  \"word_count\": 17,\n",
      "  \"char_count\": 124\n",
      "}\n",
      "\n",
      "üíæ Saved mock data to: /tmp/sample_corpus.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Sample data yang mensimulasikan corpus.jsonl format\n",
    "sample_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
    "    \"Natural language processing involves the interaction between computers and human language, enabling machines to understand text.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to solve complex problems and recognize patterns in data.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from the world around them.\",\n",
    "    \"Elasticsearch is a distributed search and analytics engine built on Apache Lucene for full-text search capabilities.\",\n",
    "    \"Vector databases store and search high-dimensional vectors efficiently, enabling semantic search and similarity matching.\",\n",
    "    \"Transformer models have revolutionized natural language processing with their attention mechanism and parallel processing.\",\n",
    "    \"Embedding models convert text into numerical representations that capture semantic meaning and context.\",\n",
    "    \"Retrieval-augmented generation combines information retrieval with language generation for improved AI responses.\",\n",
    "    \"Semantic search goes beyond keyword matching to understand the meaning and intent behind search queries.\"\n",
    "]\n",
    "\n",
    "def create_mock_jsonl_data(texts: List[str], dataset_name: str = \"sample_dataset\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create mock JSONL data similar to corpus format.\"\"\"\n",
    "    mock_data = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        doc = {\n",
    "            \"_id\": f\"{dataset_name}_chunk_{i:04d}\",\n",
    "            \"text\": text,\n",
    "            \"title\": f\"Document {i+1}\",\n",
    "            \"source\": f\"sample_source_{i+1}\",\n",
    "            \"category\": \"technology\",\n",
    "            \"chunk_index\": i,\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text)\n",
    "        }\n",
    "        mock_data.append(doc)\n",
    "    \n",
    "    return mock_data\n",
    "\n",
    "# Generate mock data\n",
    "mock_jsonl_data = create_mock_jsonl_data(sample_texts)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(mock_jsonl_data)} mock documents\")\n",
    "print(\"\\nüìÑ Sample document:\")\n",
    "print(json.dumps(mock_jsonl_data[0], indent=2))\n",
    "\n",
    "# Save to temporary file for processing simulation\n",
    "temp_jsonl_file = \"/tmp/sample_corpus.jsonl\"\n",
    "with open(temp_jsonl_file, 'w') as f:\n",
    "    for doc in mock_jsonl_data:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "print(f\"\\nüíæ Saved mock data to: {temp_jsonl_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fa1ad",
   "metadata": {},
   "source": [
    "## 5. Initialize Tokenizer and Embedding Model\n",
    "\n",
    "Setup tokenizer dan embedding model untuk text processing dan generate embeddings seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "562c44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Loaded model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:üìä Vocab size: 30522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer and embedding model initialized!\n",
      "üî§ Max tokens: 512\n",
      "üßÆ Embedding dimension: 384\n",
      "üìà Sample embedding values: [-0.16524578630924225, -0.030131183564662933, 0.13367828726768494, 0.19361034035682678, 0.12457739561796188]...\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingPipelineSimulator:\n",
    "    \"\"\"Simulated embedding pipeline based on the original code.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig, pipeline_config: Dict[str, Any]):\n",
    "        self.model_config = model_config\n",
    "        self.pipeline_config = pipeline_config\n",
    "        self.tokenizer = None\n",
    "        self.embedding_model = None\n",
    "        self.vector_stores = {}\n",
    "        \n",
    "    def initialize_tokenizer_and_model(self):\n",
    "        \"\"\"Initialize tokenizer and embedding model.\"\"\"\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_config.provider_model_id, \n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Load embedding model\n",
    "            self.embedding_model = AutoModel.from_pretrained(\n",
    "                self.model_config.provider_model_id\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"‚úÖ Loaded model: {self.model_config.provider_model_id}\")\n",
    "            logger.info(f\"üìä Vocab size: {len(self.tokenizer)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error loading model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    @staticmethod\n",
    "    def truncate_to_token_limit(text: str, tokenizer, max_tokens: int = 512) -> str:\n",
    "        \"\"\"Truncate text to token limit - same as original pipeline.\"\"\"\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_tokens,\n",
    "            return_tensors=None,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    def get_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for texts.\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                # Tokenize\n",
    "                inputs = self.tokenizer(\n",
    "                    text, \n",
    "                    return_tensors='pt', \n",
    "                    truncation=True, \n",
    "                    max_length=self.model_config.max_tokens,\n",
    "                    padding=True\n",
    "                )\n",
    "                \n",
    "                # Get embeddings\n",
    "                outputs = self.embedding_model(**inputs)\n",
    "                \n",
    "                # Mean pooling\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "                embeddings.append(embedding)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Initialize pipeline simulator\n",
    "pipeline_simulator = EmbeddingPipelineSimulator(model_config, pipeline_config)\n",
    "pipeline_simulator.initialize_tokenizer_and_model()\n",
    "\n",
    "print(\"‚úÖ Tokenizer and embedding model initialized!\")\n",
    "print(f\"üî§ Max tokens: {model_config.max_tokens}\")\n",
    "\n",
    "# Test embedding generation\n",
    "test_text = \"This is a sample text for embedding generation.\"\n",
    "test_embedding = pipeline_simulator.get_embeddings([test_text])\n",
    "print(f\"üßÆ Embedding dimension: {len(test_embedding[0])}\")\n",
    "print(f\"üìà Sample embedding values: {test_embedding[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39935f3",
   "metadata": {},
   "source": [
    "## 6. Process and Prepare Documents\n",
    "\n",
    "Load dan process documents dari mock JSONL data, handle text truncation, dan prepare metadata untuk indexing seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1216118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìÑ Processing documents from: /tmp/sample_corpus.jsonl\n",
      "INFO:__main__:‚úÖ Processed 10 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 10 chunks ready for indexing\n",
      "\n",
      "üìã Sample chunk:\n",
      "ID: sample_dataset_chunk_0000\n",
      "Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "Metadata: {'chunk_id': 'sample_dataset_chunk_0000', 'title': 'Document 1', 'source': 'sample_source_1', 'category': 'technology', 'chunk_index': 0, 'word_count': 17, 'char_count': 124}\n"
     ]
    }
   ],
   "source": [
    "def process_jsonl_documents(file_path: str, pipeline_simulator: EmbeddingPipelineSimulator) -> List[Document]:\n",
    "    \"\"\"Process JSONL documents - same logic as original pipeline.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    logger.info(f\"üìÑ Processing documents from: {file_path}\")\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # Extract metadata (exclude 'text' field)\n",
    "                metadata = {}\n",
    "                for k, v in data.items():\n",
    "                    if k == \"text\":\n",
    "                        continue\n",
    "                    if k == \"_id\":\n",
    "                        metadata[\"chunk_id\"] = v\n",
    "                    else:\n",
    "                        metadata[k] = v\n",
    "                \n",
    "                # Get text content\n",
    "                text = data.get(\"text\", \"\")\n",
    "                \n",
    "                # Apply truncation if configured\n",
    "                truncate_chunk_size = pipeline_simulator.pipeline_config.get(\"truncate_chunk_size\")\n",
    "                if truncate_chunk_size is not None:\n",
    "                    original_length = len(text.split())\n",
    "                    text = EmbeddingPipelineSimulator.truncate_to_token_limit(\n",
    "                        text=text, \n",
    "                        tokenizer=pipeline_simulator.tokenizer, \n",
    "                        max_tokens=truncate_chunk_size\n",
    "                    )\n",
    "                    new_length = len(text.split())\n",
    "                    if original_length != new_length:\n",
    "                        logger.debug(f\"üîÑ Truncated doc {line_num}: {original_length} ‚Üí {new_length} tokens\")\n",
    "                \n",
    "                # Apply text size truncation if configured\n",
    "                truncate_text_size = pipeline_simulator.pipeline_config.get(\"truncate_text_size\")\n",
    "                if truncate_text_size is not None:\n",
    "                    text = text[:truncate_text_size] if len(text) > truncate_text_size else text\n",
    "                \n",
    "                # Create document\n",
    "                document = Document(\n",
    "                    page_content=text,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(document)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"‚ùå JSON decode error at line {line_num}: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error processing line {line_num}: {e}\")\n",
    "    \n",
    "    logger.info(f\"‚úÖ Processed {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "def filter_complex_metadata(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Filter complex metadata - simplified version.\"\"\"\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        # Simple filtering - remove any non-serializable metadata\n",
    "        filtered_metadata = {}\n",
    "        for k, v in doc.metadata.items():\n",
    "            if isinstance(v, (str, int, float, bool, type(None))):\n",
    "                filtered_metadata[k] = v\n",
    "        \n",
    "        filtered_doc = Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata=filtered_metadata\n",
    "        )\n",
    "        filtered_docs.append(filtered_doc)\n",
    "    \n",
    "    return filtered_docs\n",
    "\n",
    "def documents_to_chunks(documents: List[Document]) -> List[Chunk]:\n",
    "    \"\"\"Convert documents to chunks format.\"\"\"\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunk = Chunk(\n",
    "            content=doc.page_content,\n",
    "            id=doc.metadata[\"chunk_id\"],\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process the mock JSONL data\n",
    "documents = process_jsonl_documents(temp_jsonl_file, pipeline_simulator)\n",
    "documents = filter_complex_metadata(documents)\n",
    "chunks = documents_to_chunks(documents)\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks ready for indexing\")\n",
    "print(f\"\\nüìã Sample chunk:\")\n",
    "print(f\"ID: {chunks[0].id}\")\n",
    "print(f\"Content: {chunks[0].content[:100]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b83ffc",
   "metadata": {},
   "source": [
    "## 7. Generate Document Embeddings\n",
    "\n",
    "Generate embeddings untuk semua document chunks menggunakan embedding model yang sudah dikonfigurasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9011134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üßÆ Generating embeddings for all chunks...\n",
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 10 embeddings\n",
      "‚è±Ô∏è Embedding generation took: 0.28 seconds\n",
      "üéØ Average time per document: 0.028 seconds\n",
      "üìè Embedding dimensions: 384\n",
      "üßÆ Sample embedding (first 5 values): [-0.04950002580881119, 0.07518677413463593, 0.13615085184574127, 0.203369140625, 0.05386560410261154]\n",
      "‚úÖ All embeddings have consistent dimensions: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all chunks\n",
    "logger.info(\"üßÆ Generating embeddings for all chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract text content from chunks\n",
    "chunk_texts = [chunk.content for chunk in chunks]\n",
    "\n",
    "# Generate embeddings in batches for efficiency\n",
    "batch_size = 4  # Smaller batch size for local processing\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Generating embeddings\"):\n",
    "    batch_texts = chunk_texts[i:i + batch_size]\n",
    "    batch_embeddings = pipeline_simulator.get_embeddings(batch_texts)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Generated {len(all_embeddings)} embeddings\")\n",
    "print(f\"‚è±Ô∏è Embedding generation took: {embedding_time:.2f} seconds\")\n",
    "print(f\"üéØ Average time per document: {embedding_time/len(chunks):.3f} seconds\")\n",
    "\n",
    "# Validate embeddings\n",
    "if all_embeddings:\n",
    "    embedding_dim = len(all_embeddings[0])\n",
    "    print(f\"üìè Embedding dimensions: {embedding_dim}\")\n",
    "    print(f\"üßÆ Sample embedding (first 5 values): {all_embeddings[0][:5]}\")\n",
    "    \n",
    "    # Check for consistent dimensions\n",
    "    dims_consistent = all(len(emb) == embedding_dim for emb in all_embeddings)\n",
    "    print(f\"‚úÖ All embeddings have consistent dimensions: {dims_consistent}\")\n",
    "else:\n",
    "    print(\"‚ùå No embeddings generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c471d",
   "metadata": {},
   "source": [
    "## 8. Index Documents to Elasticsearch\n",
    "\n",
    "Batch index processed documents dengan embeddings ke Elasticsearch, handle existing indices dan error management seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6c6d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìã Creating vector store with index: embedding_sample_dataset_sentence-transformers-all-minilm-l6-v2\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.050s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 1 times in a row, putting on 1 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 0 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x1565eb950>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x1565eb950>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.001s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 2 times in a row, putting on 2 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 1 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x156697d10>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156697d10>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.002s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 3 times in a row, putting on 4 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 2 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x1567408d0>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x1567408d0>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.001s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 4 times in a row, putting on 8 second timeout\n",
      "WARNING:__main__:‚ö†Ô∏è Cannot connect to Elasticsearch: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x1565eb950>: Failed to establish a new connection: [Errno 61] Connection refused))\n",
      "INFO:__main__:üîÑ Using in-memory simulation mode\n",
      "INFO:__main__:üìä Existing documents in index: 0\n",
      "INFO:__main__:üöÄ Starting batch indexing of 10 chunks...\n",
      "Indexing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 30174.85it/s]\n",
      "INFO:__main__:‚úÖ Indexing completed!\n",
      "INFO:__main__:üìä Final document count: 10\n",
      "INFO:__main__:‚è±Ô∏è Indexing took: 0.00 seconds\n",
      "INFO:__main__:üéØ Average indexing time per document: 0.000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Pipeline Statistics:\n",
      "‚îú‚îÄ‚îÄ Documents processed: 10\n",
      "‚îú‚îÄ‚îÄ Embeddings generated: 10\n",
      "‚îú‚îÄ‚îÄ Index name: embedding_sample_dataset_sentence-transformers-all-minilm-l6-v2\n",
      "‚îú‚îÄ‚îÄ Final document count: 10\n",
      "‚îî‚îÄ‚îÄ Vector store mode: Simulation\n"
     ]
    }
   ],
   "source": [
    "# Create vector store and index documents\n",
    "dataset_name = \"sample_dataset\"\n",
    "model_for_index_name = model_config.provider_model_id.replace(\"/\", \"-\")\n",
    "index_name = f\"embedding_{dataset_name.lower()}_{model_for_index_name.lower()}\"\n",
    "\n",
    "logger.info(f\"üìã Creating vector store with index: {index_name}\")\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = ElasticsearchVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding_config=model_config.model_dump()\n",
    ")\n",
    "\n",
    "# Check if index already has data (like in original pipeline)\n",
    "existing_doc_count = vector_store.count_documents()\n",
    "logger.info(f\"üìä Existing documents in index: {existing_doc_count}\")\n",
    "\n",
    "if existing_doc_count >= len(chunks):\n",
    "    logger.info(f\"‚è≠Ô∏è Index already has {existing_doc_count} documents. Skipping indexing.\")\n",
    "else:\n",
    "    logger.info(f\"üöÄ Starting batch indexing of {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Batch indexing (same as original pipeline)\n",
    "    batch_size = pipeline_config.get(\"batch_size\", 32)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Indexing batches\"):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        batch_embeddings = all_embeddings[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Add batch to vector store\n",
    "            vector_store.add_chunks_batch(batch_chunks, batch_embeddings)\n",
    "            logger.debug(f\"‚úÖ Indexed batch {i//batch_size + 1}: {len(batch_chunks)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error indexing batch {i//batch_size + 1}: {e}\")\n",
    "            # In real pipeline, this would raise the exception\n",
    "            # raise e\n",
    "    \n",
    "    indexing_time = time.time() - start_time\n",
    "    \n",
    "    # Verify indexing\n",
    "    final_doc_count = vector_store.count_documents()\n",
    "    logger.info(f\"‚úÖ Indexing completed!\")\n",
    "    logger.info(f\"üìä Final document count: {final_doc_count}\")\n",
    "    logger.info(f\"‚è±Ô∏è Indexing took: {indexing_time:.2f} seconds\")\n",
    "    logger.info(f\"üéØ Average indexing time per document: {indexing_time/len(chunks):.3f} seconds\")\n",
    "\n",
    "# Store vector store for later use\n",
    "pipeline_simulator.vector_stores[dataset_name] = vector_store\n",
    "\n",
    "print(f\"\\nüìà Pipeline Statistics:\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Documents processed: {len(chunks)}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Embeddings generated: {len(all_embeddings)}\")  \n",
    "print(f\"‚îú‚îÄ‚îÄ Index name: {index_name}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Final document count: {vector_store.count_documents()}\")\n",
    "print(f\"‚îî‚îÄ‚îÄ Vector store mode: {'Simulation' if vector_store.use_simulation else 'Elasticsearch'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66597b3e",
   "metadata": {},
   "source": [
    "## 9. Implement Semantic Search\n",
    "\n",
    "Create search functionality yang embed query text dan melakukan vector similarity search terhadap indexed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6091ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Semantic Search:\n",
      "==================================================\n",
      "\n",
      "üîé Query 1: What is machine learning and artificial intelligence?\n",
      "‚è±Ô∏è Search time: 0.001 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Score: 0.7793\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "   2. Score: 0.4836\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problems and recognize patt...\n",
      "      Chunk ID: sample_dataset_chunk_0002\n",
      "\n",
      "   3. Score: 0.4691\n",
      "      Content: computer vision allows machines to interpret and understand visual information from the world around...\n",
      "      Chunk ID: sample_dataset_chunk_0003\n",
      "\n",
      "üîé Query 2: How does natural language processing work?\n",
      "‚è±Ô∏è Search time: 0.000 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Score: 0.7726\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "   2. Score: 0.5174\n",
      "      Content: transformer models have revolutionized natural language processing with their attention mechanism an...\n",
      "      Chunk ID: sample_dataset_chunk_0006\n",
      "\n",
      "   3. Score: 0.4364\n",
      "      Content: embedding models convert text into numerical representations that capture semantic meaning and conte...\n",
      "      Chunk ID: sample_dataset_chunk_0007\n",
      "\n",
      "üîé Query 3: Tell me about deep learning and neural networks\n",
      "‚è±Ô∏è Search time: 0.000 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Score: 0.6896\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problems and recognize patt...\n",
      "      Chunk ID: sample_dataset_chunk_0002\n",
      "\n",
      "   2. Score: 0.3984\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "   3. Score: 0.3627\n",
      "      Content: computer vision allows machines to interpret and understand visual information from the world around...\n",
      "      Chunk ID: sample_dataset_chunk_0003\n",
      "\n",
      "üîé Query 4: What is vector search and embeddings?\n",
      "‚è±Ô∏è Search time: 0.000 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Score: 0.5855\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabling semantic search a...\n",
      "      Chunk ID: sample_dataset_chunk_0005\n",
      "\n",
      "   2. Score: 0.4862\n",
      "      Content: embedding models convert text into numerical representations that capture semantic meaning and conte...\n",
      "      Chunk ID: sample_dataset_chunk_0007\n",
      "\n",
      "   3. Score: 0.4203\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and intent behind search quer...\n",
      "      Chunk ID: sample_dataset_chunk_0009\n",
      "\n",
      "‚úÖ Semantic search testing completed!\n"
     ]
    }
   ],
   "source": [
    "class SemanticRetriever:\n",
    "    \"\"\"Semantic retrieval class simulating BasicVectorRetriever.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: ElasticsearchVectorStore, top_k: int = 10):\n",
    "        self.vector_store = vector_store\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        magnitude1 = sum(a * a for a in vec1) ** 0.5\n",
    "        magnitude2 = sum(b * b for b in vec2) ** 0.5\n",
    "        \n",
    "        if magnitude1 == 0 or magnitude2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    def search(self, query_embedding: List[float]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for similar documents using vector similarity.\"\"\"\n",
    "        if self.vector_store.use_simulation:\n",
    "            # Simulation mode: compute similarities in memory\n",
    "            similarities = []\n",
    "            \n",
    "            for doc in self.vector_store.simulated_docs:\n",
    "                similarity = self.cosine_similarity(query_embedding, doc['embedding'])\n",
    "                similarities.append({\n",
    "                    'chunk': doc['chunk'],\n",
    "                    'score': similarity\n",
    "                })\n",
    "            \n",
    "            # Sort by similarity score (descending)\n",
    "            similarities.sort(key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            # Return top-k results\n",
    "            return similarities[:self.top_k]\n",
    "        \n",
    "        else:\n",
    "            # Real Elasticsearch mode (would use kNN search)\n",
    "            try:\n",
    "                search_body = {\n",
    "                    \"knn\": {\n",
    "                        \"field\": \"embedding\",\n",
    "                        \"query_vector\": query_embedding,\n",
    "                        \"k\": self.top_k,\n",
    "                        \"num_candidates\": min(100, self.vector_store.count_documents())\n",
    "                    },\n",
    "                    \"_source\": [\"content\", \"metadata\", \"chunk_id\"]\n",
    "                }\n",
    "                \n",
    "                response = self.vector_store.client.search(\n",
    "                    index=self.vector_store.index_name,\n",
    "                    body=search_body\n",
    "                )\n",
    "                \n",
    "                results = []\n",
    "                for hit in response['hits']['hits']:\n",
    "                    chunk = Chunk(\n",
    "                        content=hit['_source']['content'],\n",
    "                        id=hit['_source']['chunk_id'],\n",
    "                        metadata=hit['_source']['metadata']\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'chunk': chunk,\n",
    "                        'score': hit['_score']\n",
    "                    })\n",
    "                \n",
    "                return results\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Elasticsearch search error: {e}\")\n",
    "                return []\n",
    "\n",
    "# Initialize semantic retriever\n",
    "retriever = SemanticRetriever(vector_store, top_k=pipeline_config.get(\"retrieval_top_k\", 10))\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning and artificial intelligence?\",\n",
    "    \"How does natural language processing work?\",\n",
    "    \"Tell me about deep learning and neural networks\",\n",
    "    \"What is vector search and embeddings?\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing Semantic Search:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüîé Query {i}: {query}\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = pipeline_simulator.get_embeddings([query])[0]\n",
    "    \n",
    "    # Perform search\n",
    "    search_start = time.time()\n",
    "    results = retriever.search(query_embedding)\n",
    "    search_time = time.time() - search_start\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Search time: {search_time:.3f} seconds\")\n",
    "    print(f\"üìä Found {len(results)} results\")\n",
    "    \n",
    "    # Display top 3 results\n",
    "    for j, result in enumerate(results[:3], 1):\n",
    "        chunk = result['chunk']\n",
    "        score = result['score']\n",
    "        print(f\"\\n   {j}. Score: {score:.4f}\")\n",
    "        print(f\"      Content: {chunk.content[:100]}...\")\n",
    "        print(f\"      Chunk ID: {chunk.id}\")\n",
    "\n",
    "print(\"\\n‚úÖ Semantic search testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b1fe3",
   "metadata": {},
   "source": [
    "## 10. Test Retrieval and Reranking\n",
    "\n",
    "Implement dan test document retrieval dengan optional reranking functionality, measuring retrieval accuracy dan performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90b3cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Comprehensive Retrieval Pipeline Test\n",
      "============================================================\n",
      "\n",
      "üîé Test Query 1: 'machine learning artificial intelligence'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.001s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6472\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "   2. Score: 0.4255\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "   3. Score: 0.4199\n",
      "      Content: computer vision allows machines to interpret and understand visual information f...\n",
      "      Metadata: Document 4\n",
      "\n",
      "üîé Test Query 2: 'natural language processing text analysis'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6086\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "   2. Score: 0.4426\n",
      "      Content: transformer models have revolutionized natural language processing with their at...\n",
      "      Metadata: Document 7\n",
      "   3. Score: 0.3417\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "\n",
      "üîé Test Query 3: 'deep learning neural networks patterns'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.7387\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "   2. Score: 0.3520\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "   3. Score: 0.3253\n",
      "      Content: computer vision allows machines to interpret and understand visual information f...\n",
      "      Metadata: Document 4\n",
      "\n",
      "üîé Test Query 4: 'vector database similarity search'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.001s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.8317\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "   2. Score: 0.3877\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "   3. Score: 0.2935\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "\n",
      "üîé Test Query 5: 'elasticsearch distributed search analytics'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.8391\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "   2. Score: 0.3748\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "   3. Score: 0.2533\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "\n",
      "üìà Performance Summary:\n",
      "‚îú‚îÄ‚îÄ Total search time: 0.003s\n",
      "‚îú‚îÄ‚îÄ Average search time: 0.001s\n",
      "‚îî‚îÄ‚îÄ Total queries tested: 5\n",
      "\n",
      "============================================================\n",
      "üîÑ Testing with Reranking Enabled\n",
      "üî¨ Comprehensive Retrieval Pipeline Test\n",
      "============================================================\n",
      "\n",
      "üîé Test Query 1: 'machine learning artificial intelligence'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.000s\n",
      "üîÑ Reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.9256\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "      Overlap: 1.000, Length: 0.752\n",
      "   2. Score: 0.4006\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "      Overlap: 0.250, Length: 0.752\n",
      "   3. Score: 0.2273\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "      Overlap: 0.000, Length: 0.758\n",
      "\n",
      "üîé Test Query 2: 'natural language processing text analysis'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.001s\n",
      "üîÑ Reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6439\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.600, Length: 0.746\n",
      "   2. Score: 0.6406\n",
      "      Content: transformer models have revolutionized natural language processing with their at...\n",
      "      Metadata: Document 7\n",
      "      Overlap: 0.600, Length: 0.735\n",
      "   3. Score: 0.3673\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "      Overlap: 0.200, Length: 0.758\n",
      "\n",
      "üîé Test Query 3: 'deep learning neural networks patterns'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.002s\n",
      "üîÑ Reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.9256\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "      Overlap: 1.000, Length: 0.752\n",
      "   2. Score: 0.3656\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "      Overlap: 0.200, Length: 0.752\n",
      "   3. Score: 0.2273\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "      Overlap: 0.000, Length: 0.758\n",
      "\n",
      "üîé Test Query 4: 'vector database similarity search'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.001s\n",
      "üîÑ Reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.7489\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "      Overlap: 0.750, Length: 0.746\n",
      "   2. Score: 0.4023\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "      Overlap: 0.250, Length: 0.758\n",
      "   3. Score: 0.3972\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "      Overlap: 0.250, Length: 0.741\n",
      "\n",
      "üîé Test Query 5: 'elasticsearch distributed search analytics'\n",
      "----------------------------------------\n",
      "üìä Initial retrieval: 10 results in 0.001s\n",
      "üîÑ Reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.9273\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "      Overlap: 1.000, Length: 0.758\n",
      "   2. Score: 0.3989\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "      Overlap: 0.250, Length: 0.746\n",
      "   3. Score: 0.3972\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "      Overlap: 0.250, Length: 0.741\n",
      "\n",
      "üìà Performance Summary:\n",
      "‚îú‚îÄ‚îÄ Total search time: 0.005s\n",
      "‚îú‚îÄ‚îÄ Average search time: 0.001s\n",
      "‚îú‚îÄ‚îÄ Total rerank time: 0.001s\n",
      "‚îú‚îÄ‚îÄ Average rerank time: 0.000s\n",
      "‚îî‚îÄ‚îÄ Total queries tested: 5\n"
     ]
    }
   ],
   "source": [
    "class SimpleReranker:\n",
    "    \"\"\"Simple reranker that simulates reranking functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rerank(self, query: str, chunks: List[Chunk], query_embedding: List[float]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Simple reranking based on text similarity and length preference.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Simple text-based score: keyword overlap + length penalty\n",
    "            query_words = set(query.lower().split())\n",
    "            chunk_words = set(chunk.content.lower().split())\n",
    "            \n",
    "            # Keyword overlap score\n",
    "            overlap_score = len(query_words.intersection(chunk_words)) / len(query_words) if query_words else 0\n",
    "            \n",
    "            # Length preference (prefer moderate length chunks)\n",
    "            length_score = 1.0 / (1.0 + abs(len(chunk.content.split()) - 50) * 0.01)\n",
    "            \n",
    "            # Combined score\n",
    "            rerank_score = overlap_score * 0.7 + length_score * 0.3\n",
    "            \n",
    "            results.append({\n",
    "                'chunk': chunk,\n",
    "                'score': rerank_score,\n",
    "                'overlap_score': overlap_score,\n",
    "                'length_score': length_score\n",
    "            })\n",
    "        \n",
    "        # Sort by rerank score\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return results\n",
    "\n",
    "def comprehensive_retrieval_test():\n",
    "    \"\"\"Comprehensive test of the entire retrieval pipeline.\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Comprehensive Retrieval Pipeline Test\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test configuration\n",
    "    test_queries = [\n",
    "        \"machine learning artificial intelligence\",\n",
    "        \"natural language processing text analysis\", \n",
    "        \"deep learning neural networks patterns\",\n",
    "        \"vector database similarity search\",\n",
    "        \"elasticsearch distributed search analytics\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize reranker\n",
    "    reranker = SimpleReranker()\n",
    "    \n",
    "    # Performance metrics\n",
    "    total_search_time = 0\n",
    "    total_rerank_time = 0\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîé Test Query {i}: '{query}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Step 1: Generate query embedding\n",
    "        embed_start = time.time()\n",
    "        query_embedding = pipeline_simulator.get_embeddings([query])[0]\n",
    "        embed_time = time.time() - embed_start\n",
    "        \n",
    "        # Step 2: Initial retrieval\n",
    "        search_start = time.time()\n",
    "        initial_results = retriever.search(query_embedding)\n",
    "        search_time = time.time() - search_start\n",
    "        total_search_time += search_time\n",
    "        \n",
    "        print(f\"üìä Initial retrieval: {len(initial_results)} results in {search_time:.3f}s\")\n",
    "        \n",
    "        # Step 3: Reranking (if enabled in config)\n",
    "        if pipeline_config.get(\"use_reranker\", False):\n",
    "            rerank_start = time.time()\n",
    "            \n",
    "            # Extract chunks for reranking\n",
    "            initial_chunks = [result['chunk'] for result in initial_results]\n",
    "            reranked_results = reranker.rerank(query, initial_chunks, query_embedding)\n",
    "            \n",
    "            rerank_time = time.time() - rerank_start\n",
    "            total_rerank_time += rerank_time\n",
    "            \n",
    "            print(f\"üîÑ Reranking completed in {rerank_time:.3f}s\")\n",
    "            final_results = reranked_results\n",
    "        else:\n",
    "            final_results = initial_results\n",
    "        \n",
    "        # Step 4: Display results\n",
    "        print(f\"\\nüéØ Top 3 Final Results:\")\n",
    "        for j, result in enumerate(final_results[:3], 1):\n",
    "            chunk = result['chunk']\n",
    "            score = result['score']\n",
    "            \n",
    "            print(f\"   {j}. Score: {score:.4f}\")\n",
    "            print(f\"      Content: {chunk.content[:80]}...\")\n",
    "            print(f\"      Metadata: {chunk.metadata.get('title', 'N/A')}\")\n",
    "            \n",
    "            # Show reranking details if available\n",
    "            if 'overlap_score' in result:\n",
    "                print(f\"      Overlap: {result['overlap_score']:.3f}, Length: {result['length_score']:.3f}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nüìà Performance Summary:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Total search time: {total_search_time:.3f}s\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Average search time: {total_search_time/len(test_queries):.3f}s\")\n",
    "    if total_rerank_time > 0:\n",
    "        print(f\"‚îú‚îÄ‚îÄ Total rerank time: {total_rerank_time:.3f}s\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Average rerank time: {total_rerank_time/len(test_queries):.3f}s\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Total queries tested: {len(test_queries)}\")\n",
    "\n",
    "# Run comprehensive test\n",
    "comprehensive_retrieval_test()\n",
    "\n",
    "# Test with reranking enabled\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üîÑ Testing with Reranking Enabled\")\n",
    "pipeline_config[\"use_reranker\"] = True\n",
    "comprehensive_retrieval_test()\n",
    "pipeline_config[\"use_reranker\"] = False  # Reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b893f7",
   "metadata": {},
   "source": [
    "## 11. Cleanup and Performance Monitoring\n",
    "\n",
    "Monitor index performance, document counts, dan implement cleanup procedures untuk removing test indices seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "918e3374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Performance Monitoring & Index Statistics\n",
      "============================================================\n",
      "\n",
      "üìã Dataset: sample_dataset\n",
      "‚îú‚îÄ‚îÄ Index name: embedding_sample_dataset_sentence-transformers-all-minilm-l6-v2\n",
      "‚îú‚îÄ‚îÄ Document count: 10\n",
      "‚îú‚îÄ‚îÄ Storage mode: Simulation\n",
      "‚îî‚îÄ‚îÄ Embedding dimensions: 384\n",
      "\n",
      "============================================================\n",
      "üéØ FINAL PIPELINE SUMMARY\n",
      "\n",
      "üìà Pipeline Execution Summary\n",
      "============================================================\n",
      "üèóÔ∏è Pipeline Configuration:\n",
      "‚îú‚îÄ‚îÄ Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚îú‚îÄ‚îÄ Embedding dimensions: 384\n",
      "‚îú‚îÄ‚îÄ Max tokens: 512\n",
      "‚îú‚îÄ‚îÄ Batch size: 32\n",
      "‚îú‚îÄ‚îÄ Top-K retrieval: 10\n",
      "‚îî‚îÄ‚îÄ Reranking enabled: False\n",
      "\n",
      "üìä Data Processing:\n",
      "‚îú‚îÄ‚îÄ Documents processed: 10\n",
      "‚îú‚îÄ‚îÄ Embeddings generated: 10\n",
      "‚îú‚îÄ‚îÄ Index name: embedding_sample_dataset_sentence-transformers-all-minilm-l6-v2\n",
      "‚îî‚îÄ‚îÄ Storage mode: Simulation\n",
      "\n",
      "‚úÖ Pipeline completed successfully!\n",
      "üéØ This simulation demonstrates the complete Elasticsearch embedding pipeline\n",
      "üìù All steps from the original pipeline.py have been replicated\n",
      "\n",
      "üèÅ Elasticsearch Embedding Pipeline Simulation Complete!\n",
      "üìÖ Completed at: 2025-07-24 08:29:51.031363\n",
      "üîó This notebook successfully simulates the entire embedding pipeline from pipeline.py\n"
     ]
    }
   ],
   "source": [
    "def performance_monitoring():\n",
    "    \"\"\"Monitor performance and index statistics.\"\"\"\n",
    "    \n",
    "    print(\"üìä Performance Monitoring & Index Statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Vector store statistics\n",
    "    for dataset_name, vs in pipeline_simulator.vector_stores.items():\n",
    "        print(f\"\\nüìã Dataset: {dataset_name}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Index name: {vs.index_name}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Document count: {vs.count_documents()}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Storage mode: {'Simulation' if vs.use_simulation else 'Elasticsearch'}\")\n",
    "        print(f\"‚îî‚îÄ‚îÄ Embedding dimensions: {vs.embedding_config.get('embedding_dimensions', 'Unknown')}\")\n",
    "        \n",
    "        if not vs.use_simulation and hasattr(vs, 'client'):\n",
    "            try:\n",
    "                # Get index stats from Elasticsearch\n",
    "                stats = vs.client.indices.stats(index=vs.index_name)\n",
    "                index_stats = stats['indices'][vs.index_name]['total']\n",
    "                \n",
    "                print(f\"‚îú‚îÄ‚îÄ Index size: {index_stats['store']['size_in_bytes']} bytes\")\n",
    "                print(f\"‚îú‚îÄ‚îÄ Documents indexed: {index_stats['docs']['count']}\")\n",
    "                print(f\"‚îî‚îÄ‚îÄ Search operations: {index_stats['search']['query_total']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚îú‚îÄ‚îÄ ‚ö†Ô∏è Cannot retrieve ES stats: {e}\")\n",
    "\n",
    "def cleanup_pipeline():\n",
    "    \"\"\"Cleanup vector stores and temporary files.\"\"\"\n",
    "    \n",
    "    print(\"\\nüßπ Cleanup Procedures\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    try:\n",
    "        if os.path.exists(temp_jsonl_file):\n",
    "            os.remove(temp_jsonl_file)\n",
    "            print(f\"‚úÖ Removed temporary file: {temp_jsonl_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error removing temp file: {e}\")\n",
    "    \n",
    "    # Cleanup vector stores (similar to original pipeline)\n",
    "    if pipeline_simulator.vector_stores:\n",
    "        print(f\"üóëÔ∏è Cleaning up {len(pipeline_simulator.vector_stores)} vector stores...\")\n",
    "        \n",
    "        for dataset_name, vs in pipeline_simulator.vector_stores.items():\n",
    "            try:\n",
    "                if not vs.use_simulation:\n",
    "                    # In real implementation, this would delete the index\n",
    "                    # vs.client.indices.delete(index=vs.index_name)\n",
    "                    print(f\"üóëÔ∏è Would delete index: {vs.index_name}\")\n",
    "                else:\n",
    "                    vs.simulated_docs.clear()\n",
    "                    print(f\"üóëÔ∏è Cleared simulated data for: {dataset_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error cleaning up {dataset_name}: {e}\")\n",
    "        \n",
    "        # Clear the vector stores dictionary\n",
    "        pipeline_simulator.vector_stores = {}\n",
    "        print(\"‚úÖ Vector stores cleanup completed\")\n",
    "\n",
    "def pipeline_summary():\n",
    "    \"\"\"Generate final pipeline summary.\"\"\"\n",
    "    \n",
    "    print(\"\\nüìà Pipeline Execution Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"üèóÔ∏è Pipeline Configuration:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Model: {model_config.provider_model_id}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Embedding dimensions: {model_config.embedding_dimensions}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Max tokens: {model_config.max_tokens}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Batch size: {pipeline_config.get('batch_size', 32)}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Top-K retrieval: {pipeline_config.get('retrieval_top_k', 10)}\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Reranking enabled: {pipeline_config.get('use_reranker', False)}\")\n",
    "    \n",
    "    print(f\"\\nüìä Data Processing:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Documents processed: {len(chunks)}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Embeddings generated: {len(all_embeddings)}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Index name: {index_name}\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Storage mode: {'Simulation' if vector_store.use_simulation else 'Elasticsearch'}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "    print(f\"üéØ This simulation demonstrates the complete Elasticsearch embedding pipeline\")\n",
    "    print(f\"üìù All steps from the original pipeline.py have been replicated\")\n",
    "\n",
    "# Run monitoring and summary\n",
    "performance_monitoring()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ FINAL PIPELINE SUMMARY\")\n",
    "pipeline_summary()\n",
    "\n",
    "# Optional: Run cleanup (uncomment if you want to clean up)\n",
    "# cleanup_pipeline()\n",
    "\n",
    "print(f\"\\nüèÅ Elasticsearch Embedding Pipeline Simulation Complete!\")\n",
    "print(f\"üìÖ Completed at: {datetime.now()}\")\n",
    "print(\"üîó This notebook successfully simulates the entire embedding pipeline from pipeline.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
