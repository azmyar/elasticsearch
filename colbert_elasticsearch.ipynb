{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25b9438",
   "metadata": {},
   "source": [
    "# Elasticsearch ColBERT v2 Embedding Pipeline\n",
    "\n",
    "This notebook implements a real Elasticsearch embedding pipeline using Jina ColBERT v2 embeddings with multi-vector support and Late Chunking capabilities.\n",
    "\n",
    "The pipeline performs:\n",
    "1. **Data Loading**: Reading data from JSONL format\n",
    "2. **Text Processing**: Tokenization and truncation based on ColBERT v2 token limits (8192 tokens)\n",
    "3. **Vector Indexing**: Batch indexing to Elasticsearch with multi-vector embeddings\n",
    "4. **Semantic Retrieval**: Late interaction similarity search using ColBERT methodology\n",
    "5. **Optional Reranking**: Reranking results for improved accuracy\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "JSONL Data → Text Processing → Document Creation → ColBERT Multi-Vector Embeddings → \n",
    "Elasticsearch Index (Dense Vector Storage) → Late Interaction Retrieval → \n",
    "Optional Reranking → Final Results\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Elasticsearch**: Must be running and accessible (default: localhost:9200)\n",
    "- **Jina API Key**: Required for ColBERT v2 embeddings\n",
    "- **Python packages**: elasticsearch, requests, transformers, tqdm\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Start Elasticsearch:\n",
    "   ```bash\n",
    "   docker run -d -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n",
    "   ```\n",
    "\n",
    "2. Set your Jina API key:\n",
    "   ```bash\n",
    "   export JINA_API_KEY=\"your-api-key-here\"\n",
    "   ```\n",
    "\n",
    "3. Run the notebook cells sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab1c44",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import semua library yang diperlukan untuk simulasi pipeline Elasticsearch embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b0e357e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from typing import Any, Dict, List\n",
    "import logging\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Elasticsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# HTTP requests for Jina API\n",
    "import requests\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Jina AI API Key (set your API key here)\n",
    "jinaai_key = os.environ.get(\"JINA_API_KEY\", \"jina_55815bff338d4445aae29a6f2d322ac7O-GT3q1UM_FfKyaXh2pnTKD9JyEC\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e5d33",
   "metadata": {},
   "source": [
    "## 2. Define Data Structures and Configuration Classes\n",
    "\n",
    "Mendefinisikan struktur data yang digunakan dalam pipeline embedding menggunakan **Jina ColBERT v2**, termasuk konfigurasi model dan document chunks.\n",
    "\n",
    "Pipeline ini telah diupdate untuk menggunakan **Jina ColBERT v2**\n",
    "\n",
    "### API Key Setup\n",
    "\n",
    "Sebelum menjalankan pipeline, pastikan Anda telah mengatur Jina API key:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"JINA_API_KEY\"] = \"your-jina-api-key-here\"\n",
    "```\n",
    "\n",
    "Dapatkan API key gratis di: https://jina.ai/\n",
    "\n",
    "### Architecture Comparison\n",
    "\n",
    "**Traditional Embeddings (sebelumnya):**\n",
    "```\n",
    "Text → Single Vector (384D) → Vector Search → Results\n",
    "```\n",
    "\n",
    "**ColBERT v2 (sekarang):**\n",
    "```\n",
    "Text → Multiple Token Vectors (Nx128D) → Late Interaction → Results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c665a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data structures defined successfully!\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration for Jina ColBERT v2 embedding pipeline.\"\"\"\n",
    "    provider: str = \"jinaai\"\n",
    "    provider_model_id: str = \"jina-colbert-v2\"\n",
    "    embedding_dimensions: int = 128  # ColBERT v2 uses 128 dimensions\n",
    "    max_tokens: int = 8192  # Jina ColBERT v2 supports up to 8192 tokens\n",
    "    api_endpoint: str = \"https://api.jina.ai/v1/multi-vector\"\n",
    "    input_type: str = \"document\"\n",
    "    embedding_type: str = \"float\"\n",
    "    \n",
    "    def model_dump(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Document class similar to langchain Document.\"\"\"\n",
    "    page_content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass \n",
    "class Chunk:\n",
    "    \"\"\"Chunk class from the original pipeline.\"\"\"\n",
    "    content: str\n",
    "    id: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingState:\n",
    "    \"\"\"State management for pipeline.\"\"\"\n",
    "    id: str\n",
    "    query: str\n",
    "    dataset: str\n",
    "    retrieved_chunks: List[Chunk] = None\n",
    "    reranked_chunks: List[Chunk] = None\n",
    "    supporting_facts: List[str] = None\n",
    "\n",
    "class EmbeddingStateKeys:\n",
    "    \"\"\"State keys constants.\"\"\"\n",
    "    ID = \"id\"\n",
    "    QUERY = \"query\"\n",
    "    DATASET = \"dataset\"\n",
    "    RETRIEVED_CHUNKS = \"retrieved_chunks\"\n",
    "    RERANKED_CHUNKS = \"reranked_chunks\"\n",
    "    SUPPORTING_FACTS = \"supporting_facts\"\n",
    "\n",
    "print(\"Data structures defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "77a06117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late Chunking utilities defined!\n"
     ]
    }
   ],
   "source": [
    "class LateChunkingUtils:\n",
    "    \"\"\"Utilities for implementing Late Chunking with sentence-level segmentation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_sentences(input_text: str, tokenizer) -> tuple:\n",
    "        \"\"\"\n",
    "        Split input text into sentences using tokenizer with span annotations.\n",
    "        \n",
    "        Args:\n",
    "            input_text: The text to chunk\n",
    "            tokenizer: Transformers tokenizer\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (chunks, span_annotations)\n",
    "        \"\"\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "        punctuation_mark_id = tokenizer.convert_tokens_to_ids(\".\")\n",
    "        sep_id = tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "        token_offsets = inputs[\"offset_mapping\"][0]\n",
    "        token_ids = inputs[\"input_ids\"][0]\n",
    "        \n",
    "        # Find sentence boundaries\n",
    "        chunk_positions = [\n",
    "            (i, int(start + 1))\n",
    "            for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "            if token_id == punctuation_mark_id\n",
    "            and (\n",
    "                token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "                or token_ids[i + 1] == sep_id\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Create text chunks\n",
    "        chunks = [\n",
    "            input_text[x[1] : y[1]]\n",
    "            for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "        ]\n",
    "        \n",
    "        # Create span annotations for token ranges\n",
    "        span_annotations = [\n",
    "            (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "        ]\n",
    "        \n",
    "        return chunks, span_annotations\n",
    "    \n",
    "    @staticmethod\n",
    "    def late_chunking_pooling(token_embeddings, span_annotations, max_length=None):\n",
    "        \"\"\"\n",
    "        Apply late chunking pooling to token embeddings.\n",
    "        \n",
    "        Args:\n",
    "            token_embeddings: Token-level embeddings from model\n",
    "            span_annotations: List of (start, end) token spans\n",
    "            max_length: Maximum sequence length\n",
    "            \n",
    "        Returns:\n",
    "            List of pooled chunk embeddings\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        \n",
    "        import numpy as np\n",
    "        \n",
    "        for embeddings, annotations in zip(token_embeddings, span_annotations):\n",
    "            if max_length is not None:\n",
    "                # Remove annotations beyond max length\n",
    "                annotations = [\n",
    "                    (start, min(end, max_length - 1))\n",
    "                    for (start, end) in annotations\n",
    "                    if start < (max_length - 1)\n",
    "                ]\n",
    "            \n",
    "            # Pool embeddings for each span (mean pooling)\n",
    "            pooled_embeddings = [\n",
    "                embeddings[start:end].sum(dim=0) / (end - start)\n",
    "                for start, end in annotations\n",
    "                if (end - start) >= 1\n",
    "            ]\n",
    "            \n",
    "            # Convert to numpy\n",
    "            pooled_embeddings = [\n",
    "                embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "            ]\n",
    "            outputs.append(pooled_embeddings)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "@dataclass\n",
    "class LateChunk:\n",
    "    \"\"\"Extended chunk class that includes context from late chunking.\"\"\"\n",
    "    content: str\n",
    "    id: str\n",
    "    metadata: Dict[str, Any]\n",
    "    original_text: str  # Full text that was chunked\n",
    "    span_annotation: tuple  # (start, end) token span\n",
    "    chunk_index: int  # Position in original text\n",
    "    context_aware: bool = True  # Indicates this is a late chunk\n",
    "\n",
    "print(\"Late Chunking utilities defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "95d17be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Jina API key configured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch configuration ready for ColBERT v2 with Late Chunking!\n",
      "Model: jina-colbert-v2\n",
      "Embedding dimensions: 128\n",
      "Max tokens: 8192\n",
      "API endpoint: https://api.jina.ai/v1/multi-vector\n",
      "Elasticsearch: localhost:9200\n",
      "Note: Late Chunking enabled for context-aware embeddings\n",
      "Note: ColBERT multi-vectors stored as separate documents per token\n"
     ]
    }
   ],
   "source": [
    "class ElasticsearchVectorStore:\n",
    "    \"\"\"ElasticsearchVectorDataStore class for ColBERT multi-vector embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, index_name: str, embedding_config: Dict[str, Any], es_config: Dict[str, Any] = None):\n",
    "        self.index_name = index_name\n",
    "        self.embedding_config = embedding_config\n",
    "        \n",
    "        # Use provided Elasticsearch configuration or default local setup\n",
    "        if es_config:\n",
    "            self.client = Elasticsearch([\n",
    "                {\n",
    "                    'host': es_config.get('host', 'localhost'),\n",
    "                    'port': es_config.get('port', 9200),\n",
    "                    'scheme': es_config.get('scheme', 'http')\n",
    "                }\n",
    "            ])\n",
    "        else:\n",
    "            # Default Elasticsearch connection (local)\n",
    "            self.client = Elasticsearch([\n",
    "                {'host': 'localhost', 'port': 9200, 'scheme': 'http'}\n",
    "            ])\n",
    "        \n",
    "        # Test connection - fail if Elasticsearch is not available\n",
    "        try:\n",
    "            info = self.client.info()\n",
    "            logger.info(f\"Connected to Elasticsearch: {info['version']['number']}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cannot connect to Elasticsearch: {e}\")\n",
    "            logger.error(\"Please ensure Elasticsearch is running before proceeding.\")\n",
    "            logger.info(\"To start Elasticsearch locally:\")\n",
    "            logger.info(\"   - Docker: docker run -p 9200:9200 -e 'discovery.type=single-node' elasticsearch:8.11.0\")\n",
    "            logger.info(\"   - Or download and run from: https://www.elastic.co/downloads/elasticsearch\")\n",
    "            raise ConnectionError(f\"Elasticsearch connection failed: {e}\")\n",
    "            \n",
    "        self._create_index_if_not_exists()\n",
    "    \n",
    "    def _create_index_if_not_exists(self):\n",
    "        \"\"\"Create index with proper mapping for ColBERT multi-vector search.\"\"\"\n",
    "        # For ColBERT, we'll store each token vector as a separate document\n",
    "        # with references back to the original chunk\n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \"embedding\": {  # Single vector per document\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": self.embedding_config.get(\"embedding_dimensions\", 128),\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    },\n",
    "                    \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                    \"token_index\": {\"type\": \"integer\"},  # Which token vector this is\n",
    "                    \"total_tokens\": {\"type\": \"integer\"},  # Total tokens in the chunk\n",
    "                    \"metadata\": {\"type\": \"object\"},\n",
    "                    \"timestamp\": {\"type\": \"date\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if not self.client.indices.exists(index=self.index_name):\n",
    "                self.client.indices.create(index=self.index_name, body=mapping)\n",
    "                logger.info(f\"Created index: {self.index_name}\")\n",
    "            else:\n",
    "                logger.info(f\"Index already exists: {self.index_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating index: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def count_documents(self) -> int:\n",
    "        \"\"\"Count documents in index.\"\"\"\n",
    "        try:\n",
    "            result = self.client.count(index=self.index_name)\n",
    "            return result['count']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error counting documents: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def add_chunks_batch(self, chunks: List[Chunk], embeddings: List[List[List[float]]]):\n",
    "        \"\"\"Add chunks with multi-vector embeddings in batch.\n",
    "        \n",
    "        For ColBERT, each token vector becomes a separate document in Elasticsearch.\n",
    "        \"\"\"\n",
    "        docs = []\n",
    "        \n",
    "        for chunk, token_vectors in zip(chunks, embeddings):\n",
    "            if not token_vectors:\n",
    "                continue\n",
    "                \n",
    "            # Create one document per token vector\n",
    "            for token_idx, vector in enumerate(token_vectors):\n",
    "                doc = {\n",
    "                    \"_index\": self.index_name,\n",
    "                    \"_id\": f\"{chunk.id}_token_{token_idx}\",  # Unique ID per token vector\n",
    "                    \"_source\": {\n",
    "                        \"content\": chunk.content,\n",
    "                        \"embedding\": vector,  # Single vector per document\n",
    "                        \"chunk_id\": chunk.id,\n",
    "                        \"token_index\": token_idx,\n",
    "                        \"total_tokens\": len(token_vectors),\n",
    "                        \"metadata\": chunk.metadata,\n",
    "                        \"timestamp\": datetime.now()\n",
    "                    }\n",
    "                }\n",
    "                docs.append(doc)\n",
    "        \n",
    "        # Bulk index\n",
    "        try:\n",
    "            success, failed = bulk(self.client, docs)\n",
    "            logger.info(f\"Indexed {success} token vectors, {len(failed)} failed\")\n",
    "            if failed:\n",
    "                logger.warning(f\"First failed document: {failed[0] if failed else 'None'}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Bulk indexing error: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def delete_index(self):\n",
    "        \"\"\"Delete the index.\"\"\"\n",
    "        try:\n",
    "            if self.client.indices.exists(index=self.index_name):\n",
    "                self.client.indices.delete(index=self.index_name)\n",
    "                logger.info(f\"Deleted index: {self.index_name}\")\n",
    "            else:\n",
    "                logger.info(f\"Index does not exist: {self.index_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting index: {e}\")\n",
    "            raise e\n",
    "\n",
    "# Initialize configuration\n",
    "model_config = ModelConfig()\n",
    "pipeline_config = {\n",
    "    \"vector_store_provider\": \"elasticsearch\",\n",
    "    \"chunks_file_name\": \"corpus.jsonl\",\n",
    "    \"retrieval_top_k\": 10,\n",
    "    \"truncate_chunk_size\": 8192,  # Updated for ColBERT v2\n",
    "    \"use_reranker\": False,\n",
    "    \"batch_size\": 16,  # Smaller batch size for multi-vector embeddings\n",
    "    \"enable_late_chunking\": True  # Always use Late Chunking approach\n",
    "}\n",
    "\n",
    "# Elasticsearch configuration (customize as needed)\n",
    "es_config = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 9200,\n",
    "    \"scheme\": \"http\"\n",
    "}\n",
    "\n",
    "# Validate Jina API key\n",
    "if jinaai_key == \"your-jina-api-key-here\" or not jinaai_key:\n",
    "    logger.warning(\"Jina API key not configured! Please set JINA_API_KEY environment variable\")\n",
    "    logger.info(\"You can get an API key from: https://jina.ai/\")\n",
    "else:\n",
    "    logger.info(\"Jina API key configured\")\n",
    "print(\"Elasticsearch configuration ready for ColBERT v2 with Late Chunking!\")\n",
    "print(f\"Model: {model_config.provider_model_id}\")\n",
    "print(f\"Embedding dimensions: {model_config.embedding_dimensions}\")\n",
    "print(f\"Max tokens: {model_config.max_tokens}\")\n",
    "print(f\"API endpoint: {model_config.api_endpoint}\")\n",
    "print(f\"Elasticsearch: {es_config['host']}:{es_config['port']}\")\n",
    "print(\"Note: Late Chunking enabled for context-aware embeddings\")\n",
    "print(\"Note: ColBERT multi-vectors stored as separate documents per token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06282523",
   "metadata": {},
   "source": [
    "## 3. Setup Real Elasticsearch Connection\n",
    "\n",
    "Configure and connect to Elasticsearch. This pipeline requires a running Elasticsearch instance for vector storage and retrieval.\n",
    "\n",
    "**Connection Requirements:**\n",
    "- Elasticsearch 7.0+ running on localhost:9200 (or configure custom host/port)\n",
    "- Network accessibility to the Elasticsearch cluster\n",
    "- Sufficient memory for dense vector storage\n",
    "\n",
    "**Configuration Options:**\n",
    "- **Local Development**: Default localhost:9200\n",
    "- **Docker**: Use the provided Docker command in the prerequisites\n",
    "- **Cloud**: Configure es_config with your cloud Elasticsearch endpoints\n",
    "- **Custom**: Modify es_config dictionary with your specific settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de11c9",
   "metadata": {},
   "source": [
    "## 4. Create Mock Data Structure\n",
    "\n",
    "Generate sample JSONL data similar to format corpus.jsonl untuk testing pipeline. Data ini mensimulasikan dokumen yang akan diindex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "aa7409d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 mock documents\n",
      "\n",
      "Sample document:\n",
      "{\n",
      "  \"_id\": \"sample_dataset_chunk_0000\",\n",
      "  \"text\": \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
      "  \"title\": \"Document 1\",\n",
      "  \"source\": \"sample_source_1\",\n",
      "  \"category\": \"technology\",\n",
      "  \"chunk_index\": 0,\n",
      "  \"word_count\": 17,\n",
      "  \"char_count\": 124\n",
      "}\n",
      "\n",
      "Saved mock data to: /tmp/sample_corpus.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Sample data yang mensimulasikan corpus.jsonl format\n",
    "sample_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
    "    \"Natural language processing involves the interaction between computers and human language, enabling machines to understand text.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to solve complex problems and recognize patterns in data.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from the world around them.\",\n",
    "    \"Elasticsearch is a distributed search and analytics engine built on Apache Lucene for full-text search capabilities.\",\n",
    "    \"Vector databases store and search high-dimensional vectors efficiently, enabling semantic search and similarity matching.\",\n",
    "    \"Transformer models have revolutionized natural language processing with their attention mechanism and parallel processing.\",\n",
    "    \"Embedding models convert text into numerical representations that capture semantic meaning and context.\",\n",
    "    \"Retrieval-augmented generation combines information retrieval with language generation for improved AI responses.\",\n",
    "    \"Semantic search goes beyond keyword matching to understand the meaning and intent behind search queries.\"\n",
    "]\n",
    "\n",
    "def create_mock_jsonl_data(texts: List[str], dataset_name: str = \"sample_dataset\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create mock JSONL data similar to corpus format.\"\"\"\n",
    "    mock_data = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        doc = {\n",
    "            \"_id\": f\"{dataset_name}_chunk_{i:04d}\",\n",
    "            \"text\": text,\n",
    "            \"title\": f\"Document {i+1}\",\n",
    "            \"source\": f\"sample_source_{i+1}\",\n",
    "            \"category\": \"technology\",\n",
    "            \"chunk_index\": i,\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text)\n",
    "        }\n",
    "        mock_data.append(doc)\n",
    "    \n",
    "    return mock_data\n",
    "\n",
    "# Generate mock data\n",
    "mock_jsonl_data = create_mock_jsonl_data(sample_texts)\n",
    "\n",
    "print(f\"Generated {len(mock_jsonl_data)} mock documents\")\n",
    "print(\"\\nSample document:\")\n",
    "print(json.dumps(mock_jsonl_data[0], indent=2))\n",
    "\n",
    "# Save to temporary file for processing simulation\n",
    "temp_jsonl_file = \"/tmp/sample_corpus.jsonl\"\n",
    "with open(temp_jsonl_file, 'w') as f:\n",
    "    for doc in mock_jsonl_data:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "print(f\"\\nSaved mock data to: {temp_jsonl_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fa1ad",
   "metadata": {},
   "source": [
    "## 5. Initialize Tokenizer and Embedding Model\n",
    "\n",
    "Setup tokenizer dan embedding model untuk text processing dan generate embeddings seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "562c44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Jina ColBERT v2 Pipeline with Late Chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded local Jina model for late chunking\n",
      "INFO:__main__:Initialized tokenizer for text processing\n",
      "INFO:__main__:Using Jina ColBERT v2 API for embeddings\n",
      "INFO:__main__:Late chunking enabled: True\n",
      "INFO:__main__:Vocab size: 30528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Jina ColBERT v2 pipeline initialized!\n",
      "Max tokens: 8192\n",
      "Late chunking enabled: True\n",
      "Local model loaded: True\n",
      "Tokenizer: BertTokenizerFast\n",
      "📋 Current Mode: Late Chunking (Context-Aware)\n",
      "ColBERT embedding shape: (10, 128)\n",
      "Number of token vectors: 10\n",
      "Vector dimension: 128\n",
      "Sample values from first vector: [0.14147949, 0.011352539, -0.19763184, -0.22631836, 0.12420654]\n",
      "\n",
      "Pipeline ready for Late Chunking processing!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Enhanced Jina ColBERT v2 Embedding Pipeline with Late Chunking\n",
    "print(\"Initializing Jina ColBERT v2 Pipeline with Late Chunking...\")\n",
    "\n",
    "# Ensure Late Chunking is enabled\n",
    "pipeline_config[\"enable_late_chunking\"] = True\n",
    "\n",
    "# Initialize pipeline with Late Chunking enabled\n",
    "pipeline_simulator = JinaColBERTEmbeddingPipeline(model_config, pipeline_config)\n",
    "\n",
    "try:\n",
    "    pipeline_simulator.initialize_tokenizer_and_model()\n",
    "    print(f\"Enhanced Jina ColBERT v2 pipeline initialized!\")\n",
    "    print(f\"Max tokens: {model_config.max_tokens}\")\n",
    "    print(f\"Late chunking enabled: {pipeline_simulator.late_chunking_enabled}\")\n",
    "    \n",
    "    if pipeline_simulator.late_chunking_enabled:\n",
    "        print(f\"Local model loaded: {pipeline_simulator.local_model is not None}\")\n",
    "        print(f\"Tokenizer: {type(pipeline_simulator.tokenizer).__name__}\")\n",
    "        print(\"📋 Current Mode: Late Chunking (Context-Aware)\")\n",
    "    else:\n",
    "        print(\"📋 Current Mode: Regular ColBERT\")\n",
    "    \n",
    "    # Test basic embedding generation\n",
    "    test_text = \"Machine learning enables intelligent systems.\"\n",
    "    test_embedding = pipeline_simulator.get_single_embedding(test_text, input_type=\"document\")\n",
    "    \n",
    "    if test_embedding:\n",
    "        print(f\"ColBERT embedding shape: ({len(test_embedding)}, {len(test_embedding[0]) if test_embedding else 0})\")\n",
    "        print(f\"Number of token vectors: {len(test_embedding)}\")\n",
    "        print(f\"Vector dimension: {len(test_embedding[0]) if test_embedding else 0}\")\n",
    "        print(f\"Sample values from first vector: {test_embedding[0][:5] if test_embedding else 'None'}\")\n",
    "    else:\n",
    "        print(\"Failed to generate test embedding\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing pipeline: {e}\")\n",
    "    raise e\n",
    "\n",
    "print(f\"\\nPipeline ready for Late Chunking processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39935f3",
   "metadata": {},
   "source": [
    "## 6. Process and Prepare Documents\n",
    "\n",
    "Load dan process documents dari mock JSONL data, handle text truncation, dan prepare metadata untuk indexing seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1216118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing documents with PROPER Late Chunking from: /tmp/sample_corpus.jsonl\n",
      "INFO:__main__:Processed 10 late chunks\n",
      "INFO:__main__:Context-aware chunks: 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents with TRUE Late Chunking...\n",
      "Created 10 context-aware chunks using Late Chunking\n",
      "\n",
      "Sample Late Chunk:\n",
      "ID: sample_dataset_chunk_0000_0\n",
      "Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "Context-aware: True\n",
      "Chunk index: 0\n",
      "Original text length: 124\n",
      "Span annotation: (1, 18)\n",
      "Metadata keys: ['chunk_id', 'title', 'source', 'category', 'chunk_index', 'word_count', 'char_count', 'original_chunk_id', 'late_chunk_index', 'total_chunks', 'original_text_length', 'chunk_text_length', 'context_aware']\n",
      "\n",
      "Processing method: Late Chunking (Context-Aware)\n",
      "Total chunks created: 10\n"
     ]
    }
   ],
   "source": [
    "def process_jsonl_documents_with_late_chunking(file_path: str, pipeline_simulator: JinaColBERTEmbeddingPipeline) -> List[LateChunk]:\n",
    "    \"\"\"Process JSONL documents with PROPER Late Chunking for enhanced context-aware embeddings.\"\"\"\n",
    "    late_chunks = []\n",
    "    \n",
    "    logger.info(f\"Processing documents with PROPER Late Chunking from: {file_path}\")\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # Extract metadata\n",
    "                metadata = {}\n",
    "                for k, v in data.items():\n",
    "                    if k == \"text\":\n",
    "                        continue\n",
    "                    if k == \"_id\":\n",
    "                        metadata[\"chunk_id\"] = v\n",
    "                    else:\n",
    "                        metadata[k] = v\n",
    "                \n",
    "                # Get text content\n",
    "                original_text = data.get(\"text\", \"\")\n",
    "                \n",
    "                if not original_text.strip():\n",
    "                    continue\n",
    "                \n",
    "                # Apply truncation if configured\n",
    "                truncate_chunk_size = pipeline_simulator.pipeline_config.get(\"truncate_chunk_size\")\n",
    "                if truncate_chunk_size is not None:\n",
    "                    original_length = len(original_text.split())\n",
    "                    original_text = JinaColBERTEmbeddingPipeline.truncate_to_token_limit(\n",
    "                        text=original_text, \n",
    "                        tokenizer=pipeline_simulator.tokenizer, \n",
    "                        max_tokens=truncate_chunk_size\n",
    "                    )\n",
    "                    new_length = len(original_text.split())\n",
    "                    if original_length != new_length:\n",
    "                        logger.debug(f\"🔄 Truncated doc {line_num}: {original_length} → {new_length} tokens\")\n",
    "                \n",
    "                # TRUE Late Chunking: Use sentence-level boundaries but preserve context\n",
    "                try:\n",
    "                    # Use late chunking to create context-aware chunks\n",
    "                    chunks, span_annotations = LateChunkingUtils.chunk_by_sentences(\n",
    "                        original_text, pipeline_simulator.tokenizer\n",
    "                    )\n",
    "                    \n",
    "                    # Create LateChunk objects for each chunk\n",
    "                    for i, (chunk_text, span) in enumerate(zip(chunks, span_annotations)):\n",
    "                        if chunk_text.strip():  # Only process non-empty chunks\n",
    "                            late_chunk = LateChunk(\n",
    "                                content=chunk_text.strip(),\n",
    "                                id=f\"{metadata.get('chunk_id', f'doc_{line_num}')}_{i}\",\n",
    "                                metadata={\n",
    "                                    **metadata,\n",
    "                                    \"original_chunk_id\": metadata.get('chunk_id', f'doc_{line_num}'),\n",
    "                                    \"late_chunk_index\": i,\n",
    "                                    \"total_chunks\": len(chunks),\n",
    "                                    \"original_text_length\": len(original_text),\n",
    "                                    \"chunk_text_length\": len(chunk_text),\n",
    "                                    \"context_aware\": True  # Mark as context-aware\n",
    "                                },\n",
    "                                original_text=original_text,\n",
    "                                span_annotation=span,\n",
    "                                chunk_index=i,\n",
    "                                context_aware=True\n",
    "                            )\n",
    "                            late_chunks.append(late_chunk)\n",
    "                    \n",
    "                    logger.debug(f\"Doc {line_num}: Created {len(chunks)} late chunks with context\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Late chunking failed for doc {line_num}: {e}\")\n",
    "                    # Fallback to traditional chunking\n",
    "                    late_chunk = LateChunk(\n",
    "                        content=original_text,\n",
    "                        id=metadata.get('chunk_id', f'doc_{line_num}'),\n",
    "                        metadata={**metadata, \"context_aware\": False},\n",
    "                        original_text=original_text,\n",
    "                        span_annotation=(0, len(original_text.split())),\n",
    "                        chunk_index=0,\n",
    "                        context_aware=False\n",
    "                    )\n",
    "                    late_chunks.append(late_chunk)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"JSON decode error at line {line_num}: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing line {line_num}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Processed {len(late_chunks)} late chunks\")\n",
    "    context_aware_count = sum(1 for chunk in late_chunks if chunk.context_aware)\n",
    "    logger.info(f\"Context-aware chunks: {context_aware_count}/{len(late_chunks)}\")\n",
    "    \n",
    "    return late_chunks\n",
    "\n",
    "def process_jsonl_documents(file_path: str, pipeline_simulator: JinaColBERTEmbeddingPipeline) -> List[Document]:\n",
    "    \"\"\"Process JSONL documents for regular ColBERT pipeline.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    logger.info(f\"Processing documents from: {file_path}\")\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # Extract metadata (exclude 'text' field)\n",
    "                metadata = {}\n",
    "                for k, v in data.items():\n",
    "                    if k == \"text\":\n",
    "                        continue\n",
    "                    if k == \"_id\":\n",
    "                        metadata[\"chunk_id\"] = v\n",
    "                    else:\n",
    "                        metadata[k] = v\n",
    "                \n",
    "                # Get text content\n",
    "                text = data.get(\"text\", \"\")\n",
    "                \n",
    "                # Apply truncation if configured (ColBERT v2 supports up to 8192 tokens)\n",
    "                truncate_chunk_size = pipeline_simulator.pipeline_config.get(\"truncate_chunk_size\")\n",
    "                if truncate_chunk_size is not None:\n",
    "                    original_length = len(text.split())\n",
    "                    text = JinaColBERTEmbeddingPipeline.truncate_to_token_limit(\n",
    "                        text=text, \n",
    "                        tokenizer=pipeline_simulator.tokenizer, \n",
    "                        max_tokens=truncate_chunk_size\n",
    "                    )\n",
    "                    new_length = len(text.split())\n",
    "                    if original_length != new_length:\n",
    "                        logger.debug(f\"Truncated doc {line_num}: {original_length} -> {new_length} tokens\")\n",
    "                \n",
    "                # Apply text size truncation if configured\n",
    "                truncate_text_size = pipeline_simulator.pipeline_config.get(\"truncate_text_size\")\n",
    "                if truncate_text_size is not None:\n",
    "                    text = text[:truncate_text_size] if len(text) > truncate_text_size else text\n",
    "                \n",
    "                # Create document\n",
    "                document = Document(\n",
    "                    page_content=text,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(document)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"JSON decode error at line {line_num}: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing line {line_num}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Processed {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "def filter_complex_metadata(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Filter complex metadata - simplified version.\"\"\"\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        # Simple filtering - remove any non-serializable metadata\n",
    "        filtered_metadata = {}\n",
    "        for k, v in doc.metadata.items():\n",
    "            if isinstance(v, (str, int, float, bool, type(None))):\n",
    "                filtered_metadata[k] = v\n",
    "        \n",
    "        filtered_doc = Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata=filtered_metadata\n",
    "        )\n",
    "        filtered_docs.append(filtered_doc)\n",
    "    \n",
    "    return filtered_docs\n",
    "\n",
    "def documents_to_chunks(documents: List[Document]) -> List[Chunk]:\n",
    "    \"\"\"Convert documents to chunks format.\"\"\"\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunk = Chunk(\n",
    "            content=doc.page_content,\n",
    "            id=doc.metadata[\"chunk_id\"],\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def late_chunks_to_chunks(late_chunks: List[LateChunk]) -> List[Chunk]:\n",
    "    \"\"\"Convert LateChunk objects to regular Chunk objects for compatibility.\"\"\"\n",
    "    chunks = []\n",
    "    for late_chunk in late_chunks:\n",
    "        chunk = Chunk(\n",
    "            content=late_chunk.content,\n",
    "            id=late_chunk.id,\n",
    "            metadata=late_chunk.metadata\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process the mock JSONL data based on Late Chunking setting\n",
    "if pipeline_simulator.late_chunking_enabled:\n",
    "    print(\"Processing documents with TRUE Late Chunking...\")\n",
    "    late_chunks = process_jsonl_documents_with_late_chunking(temp_jsonl_file, pipeline_simulator)\n",
    "    chunks = late_chunks_to_chunks(late_chunks)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} context-aware chunks using Late Chunking\")\n",
    "    print(f\"\\nSample Late Chunk:\")\n",
    "    sample_late_chunk = late_chunks[0]\n",
    "    print(f\"ID: {sample_late_chunk.id}\")\n",
    "    print(f\"Content: {sample_late_chunk.content[:100]}...\")\n",
    "    print(f\"Context-aware: {sample_late_chunk.context_aware}\")\n",
    "    print(f\"Chunk index: {sample_late_chunk.chunk_index}\")\n",
    "    print(f\"Original text length: {len(sample_late_chunk.original_text)}\")\n",
    "    print(f\"Span annotation: {sample_late_chunk.span_annotation}\")\n",
    "    print(f\"Metadata keys: {list(sample_late_chunk.metadata.keys())}\")\n",
    "else:\n",
    "    print(\"Processing documents with regular ColBERT chunking...\")\n",
    "    documents = process_jsonl_documents(temp_jsonl_file, pipeline_simulator)\n",
    "    documents = filter_complex_metadata(documents)\n",
    "    chunks = documents_to_chunks(documents)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks using traditional chunking\")\n",
    "    print(f\"\\nSample Regular Chunk:\")\n",
    "    sample_chunk = chunks[0]\n",
    "    print(f\"ID: {sample_chunk.id}\")\n",
    "    print(f\"Content: {sample_chunk.content[:100]}...\")\n",
    "    print(f\"Metadata keys: {list(sample_chunk.metadata.keys())}\")\n",
    "\n",
    "print(f\"\\nProcessing method: {'Late Chunking (Context-Aware)' if pipeline_simulator.late_chunking_enabled else 'Regular ColBERT'}\")\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b83ffc",
   "metadata": {},
   "source": [
    "## 7. Generate Document Embeddings with Late Chunking\n",
    "\n",
    "Generate context-aware embeddings untuk semua document chunks menggunakan Late Chunking approach. Teknik ini memproses entire document sekaligus untuk maintain context, kemudian mensegmentasi token embeddings untuk chunk-level storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9011134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating ColBERT multi-vector embeddings with Late Chunking...\n",
      "INFO:__main__:Using TRUE Late Chunking for context-aware embeddings...\n",
      "INFO:__main__:Processing 10 original documents with late chunking\n",
      "Late Chunking Embeddings:   0%|          | 0/5 [00:00<?, ?it/s]INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "Late Chunking Embeddings:  20%|██        | 1/5 [00:00<00:03,  1.10it/s]INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "Late Chunking Embeddings:  40%|████      | 2/5 [00:01<00:02,  1.35it/s]INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "Late Chunking Embeddings:  60%|██████    | 3/5 [00:02<00:01,  1.43it/s]INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "Late Chunking Embeddings:  80%|████████  | 4/5 [00:02<00:00,  1.49it/s]INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "INFO:__main__:Late chunking: 1 context-aware chunks from single encoding\n",
      "Late Chunking Embeddings: 100%|██████████| 5/5 [00:03<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 ColBERT multi-vector embeddings\n",
      "⏱Embedding generation took: 3.47 seconds\n",
      "Average time per document: 0.347 seconds\n",
      "Method: TRUE Late Chunking (Context-Aware)\n",
      "\n",
      "ColBERT embedding structure:\n",
      "├── Number of token vectors: 1\n",
      "├── Vector dimension: 20\n",
      "└── Total parameters per document: 20\n",
      "\n",
      "Token count statistics across all documents:\n",
      "├── Min tokens: 1\n",
      "├── Max tokens: 1\n",
      "├── Average tokens: 1.0\n",
      "└── Total embeddings: 10\n",
      "\n",
      "Sample values from first token vector: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "All token vectors have consistent dimensions: False\n",
      "\n",
      "Late Chunking Statistics:\n",
      "├── Context-aware embeddings: 10\n",
      "├── Failed embeddings: 0\n",
      "└── Success rate: 100.0%\n",
      "\n",
      "Late Chunking Benefits:\n",
      "├── Documents encoded as complete sequences for context\n",
      "├── Token embeddings segmented after contextualization\n",
      "└── Each chunk retains full document context information\n",
      "\n",
      "Ready for Elasticsearch indexing with context-aware ColBERT embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate ColBERT multi-vector embeddings with Late Chunking\n",
    "logger.info(\"Generating ColBERT multi-vector embeddings with Late Chunking...\")\n",
    "start_time = time.time()\n",
    "\n",
    "logger.info(\"Using TRUE Late Chunking for context-aware embeddings...\")\n",
    "\n",
    "# Group late chunks by original document for late chunking\n",
    "doc_groups = {}\n",
    "for chunk in chunks:\n",
    "    # Get original document ID from metadata\n",
    "    original_id = chunk.metadata.get('original_chunk_id', chunk.id.split('_')[0])\n",
    "    if original_id not in doc_groups:\n",
    "        doc_groups[original_id] = []\n",
    "    doc_groups[original_id].append(chunk)\n",
    "\n",
    "logger.info(f\"Processing {len(doc_groups)} original documents with late chunking\")\n",
    "\n",
    "all_embeddings = []\n",
    "doc_items = list(doc_groups.items())\n",
    "batch_size = 2  # Process fewer documents at a time for late chunking\n",
    "\n",
    "for i in tqdm(range(0, len(doc_items), batch_size), desc=\"Late Chunking Embeddings\"):\n",
    "    batch_docs = doc_items[i:i + batch_size]\n",
    "    \n",
    "    try:\n",
    "        for doc_id, doc_chunks in batch_docs:\n",
    "            # Get the original text from the first chunk\n",
    "            original_text = doc_chunks[0].metadata.get('original_text', doc_chunks[0].content)\n",
    "            if not hasattr(doc_chunks[0], 'original_text'):\n",
    "                # If LateChunk object doesn't exist, reconstruct from chunks\n",
    "                original_text = ' '.join([chunk.content for chunk in doc_chunks])\n",
    "            \n",
    "            try:\n",
    "                # Use TRUE late chunking: encode full document once, then segment\n",
    "                late_embeddings, chunks_texts, span_annotations = pipeline_simulator.get_late_chunking_embeddings(original_text)\n",
    "                \n",
    "                # Map late chunk embeddings to our chunk objects\n",
    "                for j, chunk in enumerate(doc_chunks):\n",
    "                    if j < len(late_embeddings):\n",
    "                        all_embeddings.append(late_embeddings[j])\n",
    "                    else:\n",
    "                        # If we have more chunks than embeddings, duplicate the last one\n",
    "                        if late_embeddings:\n",
    "                            all_embeddings.append(late_embeddings[-1])\n",
    "                        else:\n",
    "                            all_embeddings.append([])\n",
    "                \n",
    "                logger.debug(f\"Late chunking successful for doc {doc_id}: {len(late_embeddings)} context-aware embeddings\")\n",
    "                \n",
    "            except Exception as late_error:\n",
    "                logger.warning(f\"Late chunking failed for doc {doc_id}: {late_error}\")\n",
    "                # Fallback to individual chunk embeddings\n",
    "                for chunk in doc_chunks:\n",
    "                    try:\n",
    "                        fallback_embedding = pipeline_simulator.get_single_embedding(chunk.content, \"document\")\n",
    "                        all_embeddings.append(fallback_embedding if fallback_embedding else [])\n",
    "                    except:\n",
    "                        all_embeddings.append([])\n",
    "        \n",
    "        # Add delay to respect API rate limits\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing late chunking batch {i//batch_size + 1}: {e}\")\n",
    "        # Add empty embeddings as placeholders\n",
    "        for doc_id, doc_chunks in batch_docs:\n",
    "            for _ in doc_chunks:\n",
    "                all_embeddings.append([])\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"Generated {len(all_embeddings)} ColBERT multi-vector embeddings\")\n",
    "print(f\"⏱Embedding generation took: {embedding_time:.2f} seconds\")\n",
    "print(f\"Average time per document: {embedding_time/len(chunks):.3f} seconds\")\n",
    "print(f\"Method: TRUE Late Chunking (Context-Aware)\")\n",
    "\n",
    "# Validate ColBERT embeddings\n",
    "if all_embeddings and all_embeddings[0]:\n",
    "    first_embedding = all_embeddings[0]\n",
    "    token_count = len(first_embedding)\n",
    "    vector_dim = len(first_embedding[0]) if first_embedding else 0\n",
    "    \n",
    "    print(f\"\\nColBERT embedding structure:\")\n",
    "    print(f\"├── Number of token vectors: {token_count}\")\n",
    "    print(f\"├── Vector dimension: {vector_dim}\")\n",
    "    print(f\"└── Total parameters per document: {token_count * vector_dim}\")\n",
    "    \n",
    "    # Show stats for all embeddings\n",
    "    token_counts = [len(emb) for emb in all_embeddings if emb]\n",
    "    if token_counts:\n",
    "        print(f\"\\nToken count statistics across all documents:\")\n",
    "        print(f\"├── Min tokens: {min(token_counts)}\")\n",
    "        print(f\"├── Max tokens: {max(token_counts)}\")\n",
    "        print(f\"├── Average tokens: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "        print(f\"└── Total embeddings: {len(token_counts)}\")\n",
    "    \n",
    "    # Sample values from first embedding\n",
    "    print(f\"\\nSample values from first token vector: {first_embedding[0][:5]}\")\n",
    "    \n",
    "    # Check for consistent dimensions\n",
    "    dims_consistent = all(\n",
    "        len(emb[0]) == vector_dim for emb in all_embeddings \n",
    "        if emb and len(emb) > 0\n",
    "    )\n",
    "    print(f\"All token vectors have consistent dimensions: {dims_consistent}\")\n",
    "    \n",
    "    # Late Chunking specific statistics\n",
    "    context_aware_embeddings = len([emb for emb in all_embeddings if emb])\n",
    "    print(f\"\\nLate Chunking Statistics:\")\n",
    "    print(f\"├── Context-aware embeddings: {context_aware_embeddings}\")\n",
    "    print(f\"├── Failed embeddings: {len(all_embeddings) - context_aware_embeddings}\")\n",
    "    print(f\"└── Success rate: {(context_aware_embeddings/len(all_embeddings)*100):.1f}%\")\n",
    "    \n",
    "    # Show the key difference: context preservation\n",
    "    print(f\"\\nLate Chunking Benefits:\")\n",
    "    print(f\"├── Documents encoded as complete sequences for context\")\n",
    "    print(f\"├── Token embeddings segmented after contextualization\")\n",
    "    print(f\"└── Each chunk retains full document context information\")\n",
    "else:\n",
    "    print(\"No valid embeddings generated\")\n",
    "\n",
    "print(f\"\\nReady for Elasticsearch indexing with context-aware ColBERT embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c471d",
   "metadata": {},
   "source": [
    "## 8. Index Documents to Elasticsearch\n",
    "\n",
    "Batch index processed documents dengan embeddings ke Elasticsearch, handle existing indices dan error management seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e6c6d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating ColBERT vector store with index: colbert_sample_dataset_jina-colbert-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:200 duration:0.037s]\n",
      "INFO:__main__:Connected to Elasticsearch: 9.0.3\n",
      "INFO:elastic_transport.transport:HEAD http://localhost:9200/colbert_sample_dataset_jina-colbert-v2 [status:200 duration:0.017s]\n",
      "INFO:__main__:Index already exists: colbert_sample_dataset_jina-colbert-v2\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_count [status:200 duration:0.076s]\n",
      "INFO:__main__:Existing documents in index: 244\n",
      "INFO:__main__:Valid chunks with embeddings: 10/10\n",
      "INFO:__main__:⏭Index already has 244 documents. Skipping indexing.\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_count [status:200 duration:0.006s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ColBERT Pipeline Statistics:\n",
      "├── Documents processed: 10\n",
      "├── Valid embeddings generated: 10\n",
      "├── Failed embeddings: 0\n",
      "├── Index name: colbert_sample_dataset_jina-colbert-v2\n",
      "├── Final document count: 244\n",
      "├── Vector store mode: Real Elasticsearch\n",
      "└── Model: jina-colbert-v2 (ColBERT v2)\n",
      "\n",
      "ColBERT Embedding Statistics:\n",
      "├── Total token vectors: 10\n",
      "├── Average vectors per document: 1.0\n",
      "├── Vector dimension: 128\n",
      "└── Total parameters: 1,280\n"
     ]
    }
   ],
   "source": [
    "# Create vector store and index ColBERT multi-vector embeddings\n",
    "dataset_name = \"sample_dataset\"\n",
    "model_for_index_name = model_config.provider_model_id.replace(\"/\", \"-\")\n",
    "index_name = f\"colbert_{dataset_name.lower()}_{model_for_index_name.lower()}\"\n",
    "\n",
    "logger.info(f\"Creating ColBERT vector store with index: {index_name}\")\n",
    "\n",
    "try:\n",
    "    # Initialize vector store for ColBERT embeddings with real Elasticsearch\n",
    "    vector_store = ElasticsearchVectorStore(\n",
    "        index_name=index_name,\n",
    "        embedding_config=model_config.model_dump(),\n",
    "        es_config=es_config\n",
    "    )\n",
    "    \n",
    "    # Check if index already has data (like in original pipeline)\n",
    "    existing_doc_count = vector_store.count_documents()\n",
    "    logger.info(f\"Existing documents in index: {existing_doc_count}\")\n",
    "    \n",
    "    # Filter out chunks with empty embeddings\n",
    "    valid_chunks = []\n",
    "    valid_embeddings = []\n",
    "    for chunk, embedding in zip(chunks, all_embeddings):\n",
    "        if embedding and len(embedding) > 0:  # Check if embedding is not empty\n",
    "            valid_chunks.append(chunk)\n",
    "            valid_embeddings.append(embedding)\n",
    "    \n",
    "    logger.info(f\"Valid chunks with embeddings: {len(valid_chunks)}/{len(chunks)}\")\n",
    "    \n",
    "    if existing_doc_count >= len(valid_chunks):\n",
    "        logger.info(f\"⏭Index already has {existing_doc_count} documents. Skipping indexing.\")\n",
    "    else:\n",
    "        logger.info(f\"Starting batch indexing of {len(valid_chunks)} chunks with ColBERT embeddings...\")\n",
    "        \n",
    "        # Batch indexing for ColBERT multi-vector embeddings\n",
    "        batch_size = pipeline_config.get(\"batch_size\", 8)  # Smaller batch for multi-vector\n",
    "        start_time = time.time()\n",
    "        \n",
    "        successful_docs = 0\n",
    "        for i in tqdm(range(0, len(valid_chunks), batch_size), desc=\"Indexing ColBERT batches\"):\n",
    "            batch_chunks = valid_chunks[i:i + batch_size]\n",
    "            batch_embeddings = valid_embeddings[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Add batch to vector store\n",
    "                vector_store.add_chunks_batch(batch_chunks, batch_embeddings)\n",
    "                successful_docs += len(batch_chunks)\n",
    "                logger.debug(f\"Indexed ColBERT batch {i//batch_size + 1}: {len(batch_chunks)} chunks\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error indexing ColBERT batch {i//batch_size + 1}: {e}\")\n",
    "                # Continue with next batch instead of failing completely\n",
    "                continue\n",
    "        \n",
    "        indexing_time = time.time() - start_time\n",
    "        \n",
    "        # Verify indexing\n",
    "        final_doc_count = vector_store.count_documents()\n",
    "        logger.info(f\"ColBERT indexing completed!\")\n",
    "        logger.info(f\"Successfully indexed: {successful_docs}/{len(valid_chunks)} documents\")\n",
    "        logger.info(f\"Final document count: {final_doc_count}\")\n",
    "        logger.info(f\"Indexing took: {indexing_time:.2f} seconds\")\n",
    "        \n",
    "        if successful_docs > 0:\n",
    "            logger.info(f\"Average indexing time per document: {indexing_time/successful_docs:.3f} seconds\")\n",
    "    \n",
    "    # Store vector store for later use\n",
    "    pipeline_simulator.vector_stores[dataset_name] = vector_store\n",
    "    \n",
    "    print(f\"\\nColBERT Pipeline Statistics:\")\n",
    "    print(f\"├── Documents processed: {len(chunks)}\")\n",
    "    print(f\"├── Valid embeddings generated: {len(valid_embeddings)}\")\n",
    "    print(f\"├── Failed embeddings: {len(chunks) - len(valid_embeddings)}\")  \n",
    "    print(f\"├── Index name: {index_name}\")\n",
    "    print(f\"├── Final document count: {vector_store.count_documents()}\")\n",
    "    print(f\"├── Vector store mode: Real Elasticsearch\")\n",
    "    print(f\"└── Model: {model_config.provider_model_id} (ColBERT v2)\")\n",
    "    \n",
    "    # Display embedding statistics\n",
    "    if valid_embeddings:\n",
    "        total_vectors = sum(len(emb) for emb in valid_embeddings)\n",
    "        avg_vectors_per_doc = total_vectors / len(valid_embeddings)\n",
    "        print(f\"\\nColBERT Embedding Statistics:\")\n",
    "        print(f\"├── Total token vectors: {total_vectors:,}\")\n",
    "        print(f\"├── Average vectors per document: {avg_vectors_per_doc:.1f}\")\n",
    "        print(f\"├── Vector dimension: {model_config.embedding_dimensions}\")\n",
    "        print(f\"└── Total parameters: {total_vectors * model_config.embedding_dimensions:,}\")\n",
    "    \n",
    "except ConnectionError as e:\n",
    "    logger.error(f\"Failed to connect to Elasticsearch: {e}\")\n",
    "    logger.error(\"Please ensure Elasticsearch is running and accessible.\")\n",
    "    logger.info(\"💡 Quick start options:\")\n",
    "    logger.info(\"   1. Docker: docker run -d -p 9200:9200 -e 'discovery.type=single-node' docker.elastic.co/elasticsearch/elasticsearch:8.11.0\")\n",
    "    logger.info(\"   2. Local installation: https://www.elastic.co/downloads/elasticsearch\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during indexing: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66597b3e",
   "metadata": {},
   "source": [
    "## 9. Implement Semantic Search\n",
    "\n",
    "Create search functionality yang embed query text dan melakukan vector similarity search terhadap indexed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6091ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Enhanced ColBERT Semantic Search with Late Chunking:\n",
      "======================================================================\n",
      "\n",
      "🔎 Query 1: What is machine learning and artificial intelligence?\n",
      "Query vectors: 32 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.892s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱Search time: 1.082 seconds\n",
      "Found 10 results\n",
      "Context-aware results: 0\n",
      "Traditional results: 10\n",
      "\n",
      "   1. Late Interaction Score: 0.6881\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "   2. Late Interaction Score: 0.5795\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problems and recognize patt...\n",
      "      Chunk ID: sample_dataset_chunk_0002\n",
      "\n",
      "   3. Late Interaction Score: 0.5720\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "🔎 Query 2: How does natural language processing work with computers?\n",
      "Query vectors: 32 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.264s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱Search time: 0.395 seconds\n",
      "Found 10 results\n",
      "Context-aware results: 0\n",
      "Traditional results: 10\n",
      "\n",
      "   1. Late Interaction Score: 0.6777\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "   2. Late Interaction Score: 0.5837\n",
      "      Token Count: 22\n",
      "      Context-Aware: ❌\n",
      "      Content: transformer models have revolutionized natural language processing with their attention mechanism an...\n",
      "      Chunk ID: sample_dataset_chunk_0006\n",
      "\n",
      "   3. Late Interaction Score: 0.5535\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "🔎 Query 3: Tell me about deep learning neural networks and patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.055s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vectors: 32 tokens\n",
      "⏱Search time: 0.344 seconds\n",
      "Found 10 results\n",
      "Context-aware results: 0\n",
      "Traditional results: 10\n",
      "\n",
      "   1. Late Interaction Score: 0.6581\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problems and recognize patt...\n",
      "      Chunk ID: sample_dataset_chunk_0002\n",
      "\n",
      "   2. Late Interaction Score: 0.5334\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "   3. Late Interaction Score: 0.5106\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: retrieval - augmented generation combines information retrieval with language generation for improve...\n",
      "      Chunk ID: sample_dataset_chunk_0008\n",
      "\n",
      "🔎 Query 4: What is vector search and semantic embeddings?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.054s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vectors: 32 tokens\n",
      "⏱Search time: 0.191 seconds\n",
      "Found 10 results\n",
      "Context-aware results: 0\n",
      "Traditional results: 10\n",
      "\n",
      "   1. Late Interaction Score: 0.6367\n",
      "      Token Count: 29\n",
      "      Context-Aware: ❌\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabling semantic search a...\n",
      "      Chunk ID: sample_dataset_chunk_0005\n",
      "\n",
      "   2. Late Interaction Score: 0.6135\n",
      "      Token Count: 21\n",
      "      Context-Aware: ❌\n",
      "      Content: embedding models convert text into numerical representations that capture semantic meaning and conte...\n",
      "      Chunk ID: sample_dataset_chunk_0007\n",
      "\n",
      "   3. Late Interaction Score: 0.5941\n",
      "      Token Count: 22\n",
      "      Context-Aware: ❌\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and intent behind search quer...\n",
      "      Chunk ID: sample_dataset_chunk_0009\n",
      "\n",
      "🔎 Query 5: Explain distributed search and analytics engines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.039s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vectors: 32 tokens\n",
      "⏱Search time: 0.173 seconds\n",
      "Found 10 results\n",
      "Context-aware results: 0\n",
      "Traditional results: 10\n",
      "\n",
      "   1. Late Interaction Score: 0.6261\n",
      "      Token Count: 27\n",
      "      Context-Aware: ❌\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucene for full - text se...\n",
      "      Chunk ID: sample_dataset_chunk_0004\n",
      "\n",
      "   2. Late Interaction Score: 0.5404\n",
      "      Token Count: 22\n",
      "      Context-Aware: ❌\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and intent behind search quer...\n",
      "      Chunk ID: sample_dataset_chunk_0009\n",
      "\n",
      "   3. Late Interaction Score: 0.5353\n",
      "      Token Count: 26\n",
      "      Context-Aware: ❌\n",
      "      Content: retrieval - augmented generation combines information retrieval with language generation for improve...\n",
      "      Chunk ID: sample_dataset_chunk_0008\n",
      "\n",
      "Enhanced ColBERT semantic search with Late Chunking testing completed!\n"
     ]
    }
   ],
   "source": [
    "class ColBERTSemanticRetriever:\n",
    "    \"\"\"Enhanced ColBERT semantic retrieval class with late interaction similarity and Late Chunking support.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: ElasticsearchVectorStore, top_k: int = 10):\n",
    "        self.vector_store = vector_store\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        magnitude1 = sum(a * a for a in vec1) ** 0.5\n",
    "        magnitude2 = sum(b * b for b in vec2) ** 0.5\n",
    "        \n",
    "        if magnitude1 == 0 or magnitude2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    def late_interaction_similarity(self, query_vectors: List[List[float]], doc_vectors: List[List[float]]) -> float:\n",
    "        \"\"\"\n",
    "        Compute ColBERT late interaction similarity.\n",
    "        For each query token, find the best matching document token, then sum.\n",
    "        Enhanced for Late Chunking context-aware embeddings.\n",
    "        \"\"\"\n",
    "        if not query_vectors or not doc_vectors:\n",
    "            return 0.0\n",
    "        \n",
    "        total_score = 0.0\n",
    "        \n",
    "        # For each query token vector\n",
    "        for q_vec in query_vectors:\n",
    "            # Find the maximum similarity with any document token vector\n",
    "            max_similarity = max(\n",
    "                self.cosine_similarity(q_vec, d_vec) \n",
    "                for d_vec in doc_vectors\n",
    "            )\n",
    "            total_score += max_similarity\n",
    "        \n",
    "        # Normalize by query length (Late Chunking improvement)\n",
    "        normalized_score = total_score / len(query_vectors)\n",
    "        \n",
    "        return normalized_score\n",
    "    \n",
    "    def search(self, query_embedding: List[List[float]], include_metadata: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for similar documents using ColBERT late interaction with Late Chunking support.\"\"\"\n",
    "        try:\n",
    "            if not query_embedding or not query_embedding[0]:\n",
    "                return []\n",
    "            \n",
    "            # First, get candidate token vectors using first query vector\n",
    "            search_body = {\n",
    "                \"query\": {\n",
    "                    \"script_score\": {\n",
    "                        \"query\": {\"match_all\": {}},\n",
    "                        \"script\": {\n",
    "                            \"source\": \"Math.max(0, cosineSimilarity(params.query_vector, 'embedding') + 1.0)\",\n",
    "                            \"params\": {\n",
    "                                \"query_vector\": query_embedding[0]  # Use first vector for initial search\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"size\": 1000,  # Get many token vectors for aggregation\n",
    "                \"_source\": [\"chunk_id\", \"embedding\", \"content\", \"metadata\", \"token_index\", \"total_tokens\"]\n",
    "            }\n",
    "            \n",
    "            response = self.vector_store.client.search(\n",
    "                index=self.vector_store.index_name,\n",
    "                body=search_body\n",
    "            )\n",
    "            \n",
    "            # Group token vectors by chunk_id\n",
    "            chunk_groups = {}\n",
    "            for hit in response['hits']['hits']:\n",
    "                source = hit['_source']\n",
    "                chunk_id = source['chunk_id']\n",
    "                \n",
    "                if chunk_id not in chunk_groups:\n",
    "                    chunk_groups[chunk_id] = {\n",
    "                        'content': source['content'],\n",
    "                        'metadata': source['metadata'],\n",
    "                        'vectors': [],\n",
    "                        'chunk_id': chunk_id\n",
    "                    }\n",
    "                \n",
    "                chunk_groups[chunk_id]['vectors'].append(source['embedding'])\n",
    "            \n",
    "            # Compute late interaction scores for each chunk\n",
    "            results = []\n",
    "            for chunk_id, chunk_data in chunk_groups.items():\n",
    "                doc_vectors = chunk_data['vectors']\n",
    "                \n",
    "                if not doc_vectors:\n",
    "                    continue\n",
    "                \n",
    "                # Compute ColBERT late interaction score\n",
    "                late_interaction_score = self.late_interaction_similarity(\n",
    "                    query_embedding, doc_vectors\n",
    "                )\n",
    "                \n",
    "                chunk = Chunk(\n",
    "                    content=chunk_data['content'],\n",
    "                    id=chunk_id,\n",
    "                    metadata=chunk_data['metadata']\n",
    "                )\n",
    "                \n",
    "                result = {\n",
    "                    'chunk': chunk,\n",
    "                    'score': late_interaction_score,\n",
    "                    'late_interaction_score': late_interaction_score,\n",
    "                    'token_count': len(doc_vectors)\n",
    "                }\n",
    "                \n",
    "                # Add Late Chunking metadata if available\n",
    "                if include_metadata:\n",
    "                    metadata = chunk_data['metadata']\n",
    "                    result.update({\n",
    "                        'context_aware': metadata.get('context_aware', False),\n",
    "                        'chunk_index': metadata.get('late_chunk_index', 0),\n",
    "                        'original_text_length': metadata.get('original_text_length', 0)\n",
    "                    })\n",
    "                \n",
    "                results.append(result)\n",
    "            \n",
    "            # Sort by late interaction score\n",
    "            results.sort(key=lambda x: x['score'], reverse=True)\n",
    "            return results[:self.top_k]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Elasticsearch ColBERT search error: {e}\")\n",
    "            raise e\n",
    "\n",
    "# Initialize enhanced ColBERT semantic retriever\n",
    "retriever = ColBERTSemanticRetriever(vector_store, top_k=pipeline_config.get(\"retrieval_top_k\", 10))\n",
    "\n",
    "# Enhanced test queries for Late Chunking evaluation\n",
    "test_queries = [\n",
    "    \"What is machine learning and artificial intelligence?\",\n",
    "    \"How does natural language processing work with computers?\",\n",
    "    \"Tell me about deep learning neural networks and patterns\",\n",
    "    \"What is vector search and semantic embeddings?\",\n",
    "    \"Explain distributed search and analytics engines\"\n",
    "]\n",
    "\n",
    "print(\"Testing Enhanced ColBERT Semantic Search with Late Chunking:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n🔎 Query {i}: {query}\")\n",
    "    \n",
    "    try:\n",
    "        # Generate query embedding using ColBERT (query type)\n",
    "        query_embedding = pipeline_simulator.get_single_embedding(query, input_type=\"query\")\n",
    "        \n",
    "        if not query_embedding:\n",
    "            print(\"Failed to generate query embedding\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Query vectors: {len(query_embedding)} tokens\")\n",
    "        \n",
    "        # Perform enhanced ColBERT search with late interaction\n",
    "        search_start = time.time()\n",
    "        results = retriever.search(query_embedding, include_metadata=True)\n",
    "        search_time = time.time() - search_start\n",
    "        \n",
    "        print(f\"⏱Search time: {search_time:.3f} seconds\")\n",
    "        print(f\"Found {len(results)} results\")\n",
    "        \n",
    "        # Analyze Late Chunking vs Traditional results\n",
    "        context_aware_results = [r for r in results if r.get('context_aware', False)]\n",
    "        traditional_results = [r for r in results if not r.get('context_aware', False)]\n",
    "        \n",
    "        if pipeline_simulator.late_chunking_enabled:\n",
    "            print(f\"Context-aware results: {len(context_aware_results)}\")\n",
    "            print(f\"Traditional results: {len(traditional_results)}\")\n",
    "        \n",
    "        # Display top 3 results with enhanced information\n",
    "        for j, result in enumerate(results[:3], 1):\n",
    "            chunk = result['chunk']\n",
    "            score = result['score']\n",
    "            context_aware = result.get('context_aware', False)\n",
    "            chunk_index = result.get('chunk_index', 0)\n",
    "            token_count = result.get('token_count', 0)\n",
    "            \n",
    "            print(f\"\\n   {j}. Late Interaction Score: {score:.4f}\")\n",
    "            print(f\"      Token Count: {token_count}\")\n",
    "            print(f\"      Context-Aware: {'✅' if context_aware else '❌'}\")\n",
    "            if context_aware:\n",
    "                print(f\"      Chunk Index: {chunk_index}\")\n",
    "                print(f\"      Original Length: {result.get('original_text_length', 'N/A')} chars\")\n",
    "            print(f\"      Content: {chunk.content[:100]}...\")\n",
    "            print(f\"      Chunk ID: {chunk.id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nEnhanced ColBERT semantic search with Late Chunking testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b1fe3",
   "metadata": {},
   "source": [
    "## 10. Test Retrieval and Reranking\n",
    "\n",
    "Implement dan test document retrieval dengan optional reranking functionality, measuring retrieval accuracy dan performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1c464e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Comprehensive ColBERT Retrieval Pipeline Test\n",
      "======================================================================\n",
      "\n",
      "🔎 Test Query 1: 'machine learning'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.183s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.353s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.6568\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "   2. Score: 0.5624\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "   3. Score: 0.5402\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "\n",
      "🔎 Test Query 2: 'natural language processing text understanding'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.048s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.211s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.6488\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "   2. Score: 0.5725\n",
      "      Content: transformer models have revolutionized natural language processing with their at...\n",
      "      Metadata: Document 7\n",
      "   3. Score: 0.5440\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "\n",
      "🔎 Test Query 3: 'deep learning neural networks pattern recognition'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.038s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.197s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.6585\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "   2. Score: 0.5458\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "   3. Score: 0.5403\n",
      "      Content: retrieval - augmented generation combines information retrieval with language ge...\n",
      "      Metadata: Document 9\n",
      "\n",
      "🔎 Test Query 4: 'vector database similarity semantic search'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.053s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.190s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.6682\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "   2. Score: 0.5718\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "   3. Score: 0.5489\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "\n",
      "🔎 Test Query 5: 'elasticsearch distributed search analytics engine'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.047s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.195s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.6935\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "   2. Score: 0.5485\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "   3. Score: 0.5474\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "\n",
      "📈 ColBERT Performance Summary:\n",
      "├── Total embedding time: 7.885s\n",
      "├── Average embedding time: 1.577s\n",
      "├── Total search time: 1.145s\n",
      "├── Average search time: 0.229s\n",
      "└── Total queries tested: 5\n",
      "\n",
      "======================================================================\n",
      "🔄 Testing ColBERT with Reranking Enabled\n",
      "🔬 Comprehensive ColBERT Retrieval Pipeline Test\n",
      "======================================================================\n",
      "\n",
      "🔎 Test Query 1: 'machine learning'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.065s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.205s\n",
      "🔄 ColBERT reranking completed in 0.000s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.9120\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "      Overlap: 1.000, Length: 0.707, Diversity: 1.000\n",
      "   2. Score: 0.6620\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "      Overlap: 0.500, Length: 0.707, Diversity: 1.000\n",
      "   3. Score: 0.4113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.000, Length: 0.704, Diversity: 1.000\n",
      "\n",
      "🔎 Test Query 2: 'natural language processing text understanding'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.049s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.211s\n",
      "🔄 ColBERT reranking completed in 0.000s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.7113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.600, Length: 0.704, Diversity: 1.000\n",
      "   2. Score: 0.7098\n",
      "      Content: transformer models have revolutionized natural language processing with their at...\n",
      "      Metadata: Document 7\n",
      "      Overlap: 0.600, Length: 0.699, Diversity: 1.000\n",
      "   3. Score: 0.5091\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "      Overlap: 0.200, Length: 0.697, Diversity: 1.000\n",
      "\n",
      "🔎 Test Query 3: 'deep learning neural networks pattern recognition'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.046s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.210s\n",
      "🔄 ColBERT reranking completed in 0.000s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.7453\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "      Overlap: 0.667, Length: 0.707, Diversity: 1.000\n",
      "   2. Score: 0.4953\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "      Overlap: 0.167, Length: 0.707, Diversity: 1.000\n",
      "   3. Score: 0.4113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.000, Length: 0.704, Diversity: 1.000\n",
      "\n",
      "🔎 Test Query 4: 'vector database similarity semantic search'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.099s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.250s\n",
      "🔄 ColBERT reranking completed in 0.000s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.7863\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "      Overlap: 0.800, Length: 0.704, Diversity: 0.875\n",
      "   2. Score: 0.5972\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "      Overlap: 0.400, Length: 0.702, Diversity: 0.933\n",
      "   3. Score: 0.5091\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "      Overlap: 0.200, Length: 0.697, Diversity: 1.000\n",
      "\n",
      "🔎 Test Query 5: 'elasticsearch distributed search analytics engine'\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/colbert_sample_dataset_jina-colbert-v2/_search [status:200 duration:0.110s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: 32 token vectors\n",
      "Initial retrieval: 10 results in 0.326s\n",
      "🔄 ColBERT reranking completed in 0.000s\n",
      "\n",
      "🎯 Top 3 Final Results:\n",
      "   1. Score: 0.9017\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "      Overlap: 1.000, Length: 0.709, Diversity: 0.944\n",
      "   2. Score: 0.4972\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "      Overlap: 0.200, Length: 0.702, Diversity: 0.933\n",
      "   3. Score: 0.4863\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "      Overlap: 0.200, Length: 0.704, Diversity: 0.875\n",
      "\n",
      "📈 ColBERT Performance Summary:\n",
      "├── Total embedding time: 8.929s\n",
      "├── Average embedding time: 1.786s\n",
      "├── Total search time: 1.202s\n",
      "├── Average search time: 0.240s\n",
      "├── Total rerank time: 0.000s\n",
      "├── Average rerank time: 0.000s\n",
      "└── Total queries tested: 5\n"
     ]
    }
   ],
   "source": [
    "class ColBERTReranker:\n",
    "    \"\"\"ColBERT-aware reranker that considers token-level interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rerank(self, query: str, chunks: List[Chunk], query_embedding: List[List[float]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Rerank using ColBERT late interaction + text-based features.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Text-based scoring features\n",
    "            query_words = set(query.lower().split())\n",
    "            chunk_words = set(chunk.content.lower().split())\n",
    "            \n",
    "            # Keyword overlap score\n",
    "            overlap_score = len(query_words.intersection(chunk_words)) / len(query_words) if query_words else 0\n",
    "            \n",
    "            # Length preference (prefer moderate length chunks)\n",
    "            length_score = 1.0 / (1.0 + abs(len(chunk.content.split()) - 100) * 0.005)\n",
    "            \n",
    "            # Diversity score (prefer chunks with varied vocabulary)\n",
    "            diversity_score = len(chunk_words) / len(chunk.content.split()) if chunk.content.split() else 0\n",
    "            \n",
    "            # Combined rerank score\n",
    "            rerank_score = overlap_score * 0.5 + length_score * 0.3 + diversity_score * 0.2\n",
    "            \n",
    "            results.append({\n",
    "                'chunk': chunk,\n",
    "                'score': rerank_score,\n",
    "                'overlap_score': overlap_score,\n",
    "                'length_score': length_score,\n",
    "                'diversity_score': diversity_score\n",
    "            })\n",
    "        \n",
    "        # Sort by rerank score\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return results\n",
    "\n",
    "def comprehensive_colbert_retrieval_test():\n",
    "    \"\"\"Comprehensive test of the ColBERT retrieval pipeline.\"\"\"\n",
    "    \n",
    "    print(\"🔬 Comprehensive ColBERT Retrieval Pipeline Test\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test configuration\n",
    "    test_queries = [\n",
    "        \"machine learning\",\n",
    "        \"natural language processing text understanding\", \n",
    "        \"deep learning neural networks pattern recognition\",\n",
    "        \"vector database similarity semantic search\",\n",
    "        \"elasticsearch distributed search analytics engine\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize ColBERT reranker\n",
    "    reranker = ColBERTReranker()\n",
    "    \n",
    "    # Performance metrics\n",
    "    total_embedding_time = 0\n",
    "    total_search_time = 0\n",
    "    total_rerank_time = 0\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n🔎 Test Query {i}: '{query}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Generate query embedding (ColBERT multi-vector)\n",
    "            embed_start = time.time()\n",
    "            query_embedding = pipeline_simulator.get_single_embedding(query, input_type=\"query\")\n",
    "            embed_time = time.time() - embed_start\n",
    "            total_embedding_time += embed_time\n",
    "            \n",
    "            if not query_embedding:\n",
    "                print(\"Failed to generate query embedding\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Query embedding: {len(query_embedding)} token vectors\")\n",
    "            \n",
    "            # Step 2: Initial ColBERT retrieval with late interaction\n",
    "            search_start = time.time()\n",
    "            initial_results = retriever.search(query_embedding)\n",
    "            search_time = time.time() - search_start\n",
    "            total_search_time += search_time\n",
    "            \n",
    "            print(f\"Initial retrieval: {len(initial_results)} results in {search_time:.3f}s\")\n",
    "            \n",
    "            # Step 3: Reranking (if enabled in config)\n",
    "            if pipeline_config.get(\"use_reranker\", False):\n",
    "                rerank_start = time.time()\n",
    "                \n",
    "                # Extract chunks for reranking\n",
    "                initial_chunks = [result['chunk'] for result in initial_results]\n",
    "                reranked_results = reranker.rerank(query, initial_chunks, query_embedding)\n",
    "                \n",
    "                rerank_time = time.time() - rerank_start\n",
    "                total_rerank_time += rerank_time\n",
    "                \n",
    "                print(f\"🔄 ColBERT reranking completed in {rerank_time:.3f}s\")\n",
    "                final_results = reranked_results\n",
    "            else:\n",
    "                final_results = initial_results\n",
    "            \n",
    "            # Step 4: Display results\n",
    "            print(f\"\\n🎯 Top 3 Final Results:\")\n",
    "            for j, result in enumerate(final_results[:3], 1):\n",
    "                chunk = result['chunk']\n",
    "                score = result['score']\n",
    "                \n",
    "                print(f\"   {j}. Score: {score:.4f}\")\n",
    "                print(f\"      Content: {chunk.content[:80]}...\")\n",
    "                print(f\"      Metadata: {chunk.metadata.get('title', 'N/A')}\")\n",
    "                \n",
    "                # Show reranking details if available\n",
    "                if 'overlap_score' in result:\n",
    "                    print(f\"      Overlap: {result['overlap_score']:.3f}, Length: {result['length_score']:.3f}, Diversity: {result['diversity_score']:.3f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\n📈 ColBERT Performance Summary:\")\n",
    "    print(f\"├── Total embedding time: {total_embedding_time:.3f}s\")\n",
    "    print(f\"├── Average embedding time: {total_embedding_time/len(test_queries):.3f}s\")\n",
    "    print(f\"├── Total search time: {total_search_time:.3f}s\")\n",
    "    print(f\"├── Average search time: {total_search_time/len(test_queries):.3f}s\")\n",
    "    if total_rerank_time > 0:\n",
    "        print(f\"├── Total rerank time: {total_rerank_time:.3f}s\")\n",
    "        print(f\"├── Average rerank time: {total_rerank_time/len(test_queries):.3f}s\")\n",
    "    print(f\"└── Total queries tested: {len(test_queries)}\")\n",
    "\n",
    "# Run comprehensive ColBERT test\n",
    "comprehensive_colbert_retrieval_test()\n",
    "\n",
    "# Test with reranking enabled\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"🔄 Testing ColBERT with Reranking Enabled\")\n",
    "pipeline_config[\"use_reranker\"] = True\n",
    "comprehensive_colbert_retrieval_test()\n",
    "pipeline_config[\"use_reranker\"] = False  # Reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b893f7",
   "metadata": {},
   "source": [
    "## 11. Cleanup\n",
    "\n",
    "Implement cleanup procedures untuk removing test indices seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "918e3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_pipeline():\n",
    "    \"\"\"Cleanup enhanced vector stores and temporary files.\"\"\"\n",
    "    \n",
    "    print(\"\\nEnhanced ColBERT + Late Chunking Pipeline Cleanup\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    try:\n",
    "        if os.path.exists(temp_jsonl_file):\n",
    "            os.remove(temp_jsonl_file)\n",
    "            print(f\"Removed temporary file: {temp_jsonl_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error removing temp file: {e}\")\n",
    "    \n",
    "    # Cleanup vector stores and Elasticsearch indices\n",
    "    if pipeline_simulator.vector_stores:\n",
    "        print(f\"Cleaning up {len(pipeline_simulator.vector_stores)} enhanced vector stores...\")\n",
    "        \n",
    "        indices_to_clean = []\n",
    "        for dataset_name, vs in pipeline_simulator.vector_stores.items():\n",
    "            try:\n",
    "                # Delete the actual Elasticsearch index\n",
    "                vs.delete_index()\n",
    "                indices_to_clean.append(vs.index_name)\n",
    "                print(f\"Deleted Elasticsearch index: {vs.index_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning up {dataset_name}: {e}\")\n",
    "        \n",
    "        # Clear the vector stores dictionary\n",
    "        pipeline_simulator.vector_stores = {}\n",
    "        print(\"Enhanced vector stores cleanup completed\")\n",
    "        \n",
    "        if indices_to_clean:\n",
    "            print(f\"📊 Cleaned up {len(indices_to_clean)} Elasticsearch indices:\")\n",
    "            for idx in indices_to_clean:\n",
    "                print(f\"   - {idx}\")\n",
    "    else:\n",
    "        print(\"ℹ️ No vector stores to clean up\")\n",
    "\n",
    "# cleanup_pipeline() uncomment to run cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
