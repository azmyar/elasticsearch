{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25b9438",
   "metadata": {},
   "source": [
    "# Elasticsearch Embedding Pipeline Simulation\n",
    "\n",
    "Notebook ini mensimulasikan pipeline embedding Elasticsearch berdasarkan kode dari `benchmarks/embedding/contrib/pipeline/pipeline.py`. \n",
    "\n",
    "Pipeline ini melakukan:\n",
    "1. **Data Loading**: Membaca data dari format JSONL\n",
    "2. **Text Processing**: Tokenisasi dan truncation berdasarkan token limit\n",
    "3. **Vector Indexing**: Batch indexing ke Elasticsearch dengan embeddings\n",
    "4. **Semantic Retrieval**: Pencarian semantik dengan similarity vector\n",
    "5. **Optional Reranking**: Reranking hasil retrieval untuk akurasi yang lebih baik\n",
    "\n",
    "## Overview Pipeline Architecture\n",
    "\n",
    "```\n",
    "JSONL Data ‚Üí Text Processing ‚Üí Document Creation ‚Üí Chunk Conversion ‚Üí \n",
    "Elasticsearch Index (Vector Embeddings + Metadata) ‚Üí Vector Retrieval ‚Üí \n",
    "Optional Reranking ‚Üí Final Results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab1c44",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import semua library yang diperlukan untuk simulasi pipeline Elasticsearch embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b0e357e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Timestamp: 2025-07-24 09:17:32.846767\n",
      "üîë Jina API Key configured: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Elasticsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# HTTP requests for Jina API\n",
    "import requests\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Jina AI API Key (set your API key here)\n",
    "jinaai_key = os.environ.get(\"JINA_API_KEY\", \"jina_55815bff338d4445aae29a6f2d322ac7O-GT3q1UM_FfKyaXh2pnTKD9JyEC\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Timestamp: {datetime.now()}\")\n",
    "print(f\"üîë Jina API Key configured: {'‚úÖ' if jinaai_key != 'your-jina-api-key-here' else '‚ùå Please set JINA_API_KEY environment variable'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e5d33",
   "metadata": {},
   "source": [
    "## 2. Define Data Structures and Configuration Classes\n",
    "\n",
    "Mendefinisikan struktur data yang digunakan dalam pipeline embedding menggunakan **Jina ColBERT v2**, termasuk konfigurasi model dan document chunks.\n",
    "\n",
    "### üöÄ Model Migration: Jina ColBERT v2\n",
    "\n",
    "Pipeline ini telah diupdate untuk menggunakan **Jina ColBERT v2** yang memberikan keunggulan:\n",
    "\n",
    "1. **Multi-Vector Architecture**: Setiap dokumen direpresentasikan sebagai multiple token-level vectors\n",
    "2. **Late Interaction**: Similarity dihitung melalui token-to-token matching yang lebih presisi\n",
    "3. **Higher Token Limit**: Mendukung hingga 8192 tokens (vs 512 tokens sebelumnya)\n",
    "4. **Better Semantic Understanding**: Lebih akurat untuk dokumen panjang dan complex queries\n",
    "\n",
    "### üîë API Key Setup\n",
    "\n",
    "Sebelum menjalankan pipeline, pastikan Anda telah mengatur Jina API key:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"JINA_API_KEY\"] = \"your-jina-api-key-here\"\n",
    "```\n",
    "\n",
    "Dapatkan API key gratis di: https://jina.ai/\n",
    "\n",
    "### üèóÔ∏è Architecture Comparison\n",
    "\n",
    "**Traditional Embeddings (sebelumnya):**\n",
    "```\n",
    "Text ‚Üí Single Vector (384D) ‚Üí Vector Search ‚Üí Results\n",
    "```\n",
    "\n",
    "**ColBERT v2 (sekarang):**\n",
    "```\n",
    "Text ‚Üí Multiple Token Vectors (Nx128D) ‚Üí Late Interaction ‚Üí Results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c665a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data structures defined successfully!\n",
      "üîÑ Updated to use Jina ColBERT v2 model\n",
      "üìè Embedding dimensions: 128\n",
      "üî§ Max tokens: 8192\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration for Jina ColBERT v2 embedding pipeline.\"\"\"\n",
    "    provider: str = \"jinaai\"\n",
    "    provider_model_id: str = \"jina-colbert-v2\"\n",
    "    embedding_dimensions: int = 128  # ColBERT v2 uses 128 dimensions\n",
    "    max_tokens: int = 8192  # Jina ColBERT v2 supports up to 8192 tokens\n",
    "    api_endpoint: str = \"https://api.jina.ai/v1/multi-vector\"\n",
    "    input_type: str = \"document\"\n",
    "    embedding_type: str = \"float\"\n",
    "    \n",
    "    def model_dump(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Document class similar to langchain Document.\"\"\"\n",
    "    page_content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass \n",
    "class Chunk:\n",
    "    \"\"\"Chunk class from the original pipeline.\"\"\"\n",
    "    content: str\n",
    "    id: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingState:\n",
    "    \"\"\"State management for pipeline.\"\"\"\n",
    "    id: str\n",
    "    query: str\n",
    "    dataset: str\n",
    "    retrieved_chunks: List[Chunk] = None\n",
    "    reranked_chunks: List[Chunk] = None\n",
    "    supporting_facts: List[str] = None\n",
    "\n",
    "class EmbeddingStateKeys:\n",
    "    \"\"\"State keys constants.\"\"\"\n",
    "    ID = \"id\"\n",
    "    QUERY = \"query\"\n",
    "    DATASET = \"dataset\"\n",
    "    RETRIEVED_CHUNKS = \"retrieved_chunks\"\n",
    "    RERANKED_CHUNKS = \"reranked_chunks\"\n",
    "    SUPPORTING_FACTS = \"supporting_facts\"\n",
    "\n",
    "print(\"‚úÖ Data structures defined successfully!\")\n",
    "print(\"üîÑ Updated to use Jina ColBERT v2 model\")\n",
    "print(f\"üìè Embedding dimensions: 128\")\n",
    "print(f\"üî§ Max tokens: 8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06282523",
   "metadata": {},
   "source": [
    "## 3. Setup Elasticsearch Connection\n",
    "\n",
    "Konfigurasi dan koneksi ke Elasticsearch. Untuk simulasi, kita akan menggunakan koneksi lokal atau Docker Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8bfe2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Jina API key configured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Elasticsearch configuration ready for ColBERT v2!\n",
      "üìù Model: jina-colbert-v2\n",
      "üîç Embedding dimensions: 128\n",
      "üî§ Max tokens: 8192\n",
      "üåê API endpoint: https://api.jina.ai/v1/multi-vector\n"
     ]
    }
   ],
   "source": [
    "class ElasticsearchVectorStore:\n",
    "    \"\"\"Simulated ElasticsearchVectorDataStore class for ColBERT multi-vector embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, index_name: str, embedding_config: Dict[str, Any]):\n",
    "        self.index_name = index_name\n",
    "        self.embedding_config = embedding_config\n",
    "        \n",
    "        # Elasticsearch connection (sesuaikan dengan setup lokal)\n",
    "        self.client = Elasticsearch([\n",
    "            {'host': 'localhost', 'port': 9200, 'scheme': 'http'}\n",
    "        ])\n",
    "        \n",
    "        # Untuk simulasi, kita akan menggunakan in-memory storage jika ES tidak tersedia\n",
    "        self.use_simulation = False\n",
    "        self.simulated_docs = []\n",
    "        \n",
    "        try:\n",
    "            # Test connection\n",
    "            info = self.client.info()\n",
    "            logger.info(f\"‚úÖ Connected to Elasticsearch: {info['version']['number']}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Cannot connect to Elasticsearch: {e}\")\n",
    "            logger.info(\"üîÑ Using in-memory simulation mode\")\n",
    "            self.use_simulation = True\n",
    "            \n",
    "        self._create_index_if_not_exists()\n",
    "    \n",
    "    def _create_index_if_not_exists(self):\n",
    "        \"\"\"Create index with proper mapping for ColBERT multi-vector search.\"\"\"\n",
    "        if self.use_simulation:\n",
    "            return\n",
    "            \n",
    "        # Updated mapping for ColBERT multi-vector embeddings\n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \"embeddings\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": self.embedding_config.get(\"embedding_dimensions\", 128),\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    },\n",
    "                    \"metadata\": {\"type\": \"object\"},\n",
    "                    \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                    \"timestamp\": {\"type\": \"date\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if not self.client.indices.exists(index=self.index_name):\n",
    "                self.client.indices.create(index=self.index_name, body=mapping)\n",
    "                logger.info(f\"‚úÖ Created index: {self.index_name}\")\n",
    "            else:\n",
    "                logger.info(f\"üìã Index already exists: {self.index_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error creating index: {e}\")\n",
    "    \n",
    "    def count_documents(self) -> int:\n",
    "        \"\"\"Count documents in index.\"\"\"\n",
    "        if self.use_simulation:\n",
    "            return len(self.simulated_docs)\n",
    "        \n",
    "        try:\n",
    "            result = self.client.count(index=self.index_name)\n",
    "            return result['count']\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def add_chunks_batch(self, chunks: List[Chunk], embeddings: List[List[List[float]]]):\n",
    "        \"\"\"Add chunks with multi-vector embeddings in batch.\"\"\"\n",
    "        if self.use_simulation:\n",
    "            for chunk, embedding in zip(chunks, embeddings):\n",
    "                self.simulated_docs.append({\n",
    "                    'chunk': chunk,\n",
    "                    'embedding': embedding  # Multi-vector embedding\n",
    "                })\n",
    "            return\n",
    "        \n",
    "        # Prepare documents for bulk indexing\n",
    "        docs = []\n",
    "        for chunk, embedding in zip(chunks, embeddings):\n",
    "            doc = {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_id\": chunk.id,\n",
    "                \"_source\": {\n",
    "                    \"content\": chunk.content,\n",
    "                    \"embeddings\": embedding,  # Store multi-vector embeddings\n",
    "                    \"metadata\": chunk.metadata,\n",
    "                    \"chunk_id\": chunk.id,\n",
    "                    \"timestamp\": datetime.now()\n",
    "                }\n",
    "            }\n",
    "            docs.append(doc)\n",
    "        \n",
    "        # Bulk index\n",
    "        try:\n",
    "            success, failed = bulk(self.client, docs)\n",
    "            logger.info(f\"‚úÖ Indexed {success} documents, {len(failed)} failed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Bulk indexing error: {e}\")\n",
    "\n",
    "# Initialize configuration\n",
    "model_config = ModelConfig()\n",
    "pipeline_config = {\n",
    "    \"vector_store_provider\": \"elasticsearch\",\n",
    "    \"chunks_file_name\": \"corpus.jsonl\",\n",
    "    \"retrieval_top_k\": 10,\n",
    "    \"truncate_chunk_size\": 8192,  # Updated for ColBERT v2\n",
    "    \"use_reranker\": False,\n",
    "    \"batch_size\": 16  # Smaller batch size for multi-vector embeddings\n",
    "}\n",
    "\n",
    "# Validate Jina API key\n",
    "if jinaai_key == \"your-jina-api-key-here\" or not jinaai_key:\n",
    "    logger.warning(\"‚ö†Ô∏è Jina API key not configured! Please set JINA_API_KEY environment variable\")\n",
    "    logger.info(\"üí° You can get an API key from: https://jina.ai/\")\n",
    "else:\n",
    "    logger.info(\"‚úÖ Jina API key configured\")\n",
    "\n",
    "print(\"‚úÖ Elasticsearch configuration ready for ColBERT v2!\")\n",
    "print(f\"üìù Model: {model_config.provider_model_id}\")\n",
    "print(f\"üîç Embedding dimensions: {model_config.embedding_dimensions}\")\n",
    "print(f\"üî§ Max tokens: {model_config.max_tokens}\")\n",
    "print(f\"üåê API endpoint: {model_config.api_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de11c9",
   "metadata": {},
   "source": [
    "## 4. Create Mock Data Structure\n",
    "\n",
    "Generate sample JSONL data similar to format corpus.jsonl untuk testing pipeline. Data ini mensimulasikan dokumen yang akan diindex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aa7409d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 10 mock documents\n",
      "\n",
      "üìÑ Sample document:\n",
      "{\n",
      "  \"_id\": \"sample_dataset_chunk_0000\",\n",
      "  \"text\": \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
      "  \"title\": \"Document 1\",\n",
      "  \"source\": \"sample_source_1\",\n",
      "  \"category\": \"technology\",\n",
      "  \"chunk_index\": 0,\n",
      "  \"word_count\": 17,\n",
      "  \"char_count\": 124\n",
      "}\n",
      "\n",
      "üíæ Saved mock data to: /tmp/sample_corpus.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Sample data yang mensimulasikan corpus.jsonl format\n",
    "sample_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
    "    \"Natural language processing involves the interaction between computers and human language, enabling machines to understand text.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to solve complex problems and recognize patterns in data.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from the world around them.\",\n",
    "    \"Elasticsearch is a distributed search and analytics engine built on Apache Lucene for full-text search capabilities.\",\n",
    "    \"Vector databases store and search high-dimensional vectors efficiently, enabling semantic search and similarity matching.\",\n",
    "    \"Transformer models have revolutionized natural language processing with their attention mechanism and parallel processing.\",\n",
    "    \"Embedding models convert text into numerical representations that capture semantic meaning and context.\",\n",
    "    \"Retrieval-augmented generation combines information retrieval with language generation for improved AI responses.\",\n",
    "    \"Semantic search goes beyond keyword matching to understand the meaning and intent behind search queries.\"\n",
    "]\n",
    "\n",
    "def create_mock_jsonl_data(texts: List[str], dataset_name: str = \"sample_dataset\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create mock JSONL data similar to corpus format.\"\"\"\n",
    "    mock_data = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        doc = {\n",
    "            \"_id\": f\"{dataset_name}_chunk_{i:04d}\",\n",
    "            \"text\": text,\n",
    "            \"title\": f\"Document {i+1}\",\n",
    "            \"source\": f\"sample_source_{i+1}\",\n",
    "            \"category\": \"technology\",\n",
    "            \"chunk_index\": i,\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text)\n",
    "        }\n",
    "        mock_data.append(doc)\n",
    "    \n",
    "    return mock_data\n",
    "\n",
    "# Generate mock data\n",
    "mock_jsonl_data = create_mock_jsonl_data(sample_texts)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(mock_jsonl_data)} mock documents\")\n",
    "print(\"\\nüìÑ Sample document:\")\n",
    "print(json.dumps(mock_jsonl_data[0], indent=2))\n",
    "\n",
    "# Save to temporary file for processing simulation\n",
    "temp_jsonl_file = \"/tmp/sample_corpus.jsonl\"\n",
    "with open(temp_jsonl_file, 'w') as f:\n",
    "    for doc in mock_jsonl_data:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "print(f\"\\nüíæ Saved mock data to: {temp_jsonl_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fa1ad",
   "metadata": {},
   "source": [
    "## 5. Initialize Tokenizer and Embedding Model\n",
    "\n",
    "Setup tokenizer dan embedding model untuk text processing dan generate embeddings seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "562c44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Initialized tokenizer for text processing\n",
      "INFO:__main__:üåê Using Jina ColBERT v2 API for embeddings\n",
      "INFO:__main__:üìä Vocab size: 30522\n",
      "INFO:__main__:üß™ Testing ColBERT embedding generation...\n",
      "INFO:__main__:üåê Using Jina ColBERT v2 API for embeddings\n",
      "INFO:__main__:üìä Vocab size: 30522\n",
      "INFO:__main__:üß™ Testing ColBERT embedding generation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Jina ColBERT v2 pipeline initialized!\n",
      "üî§ Max tokens: 8192\n",
      "üßÆ ColBERT embedding shape: (16, 128)\n",
      "üìà Number of token vectors: 16\n",
      "üìè Vector dimension: 128\n",
      "üìä Sample values from first vector: [0.112854004, 0.080444336, -0.15185547, -0.13452148, 0.0670166]\n",
      "üßÆ ColBERT embedding shape: (16, 128)\n",
      "üìà Number of token vectors: 16\n",
      "üìè Vector dimension: 128\n",
      "üìä Sample values from first vector: [0.112854004, 0.080444336, -0.15185547, -0.13452148, 0.0670166]\n"
     ]
    }
   ],
   "source": [
    "class JinaColBERTEmbeddingPipeline:\n",
    "    \"\"\"Jina ColBERT v2 embedding pipeline based on the original code and ColBERT reference.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig, pipeline_config: Dict[str, Any]):\n",
    "        self.model_config = model_config\n",
    "        self.pipeline_config = pipeline_config\n",
    "        self.api_key = jinaai_key\n",
    "        self.tokenizer = None\n",
    "        self.vector_stores = {}\n",
    "        \n",
    "        if not self.api_key or self.api_key == \"your-jina-api-key-here\":\n",
    "            raise ValueError(\"Jina API key is required! Please set JINA_API_KEY environment variable\")\n",
    "    \n",
    "    def initialize_tokenizer_and_model(self):\n",
    "        \"\"\"Initialize tokenizer for text processing (no local model needed for Jina API).\"\"\"\n",
    "        try:\n",
    "            # Load tokenizer for text truncation and processing\n",
    "            # We'll use a simple tokenizer for text processing, actual embedding is done via API\n",
    "            from transformers import AutoTokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"sentence-transformers/all-MiniLM-L6-v2\",  # Use for tokenization only\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"‚úÖ Initialized tokenizer for text processing\")\n",
    "            logger.info(f\"üåê Using Jina ColBERT v2 API for embeddings\")\n",
    "            logger.info(f\"üìä Vocab size: {len(self.tokenizer)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    @staticmethod\n",
    "    def truncate_to_token_limit(text: str, tokenizer, max_tokens: int = 8192) -> str:\n",
    "        \"\"\"Truncate text to token limit for ColBERT v2.\"\"\"\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_tokens,\n",
    "            return_tensors=None,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    def get_colbert_embeddings(self, texts: List[str], input_type: str = \"document\") -> List[List[List[float]]]:\n",
    "        \"\"\"Generate ColBERT multi-vector embeddings using Jina API.\"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "        }\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            data = {\n",
    "                \"model\": self.model_config.provider_model_id,\n",
    "                \"dimensions\": self.model_config.embedding_dimensions,\n",
    "                \"input_type\": input_type,  # \"document\" or \"query\"\n",
    "                \"embedding_type\": self.model_config.embedding_type,\n",
    "                \"input\": [text],\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.model_config.api_endpoint,\n",
    "                    headers=headers,\n",
    "                    data=json.dumps(data),\n",
    "                    timeout=30\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                response_data = response.json()\n",
    "                embedding = response_data[\"data\"][0][\"embeddings\"]\n",
    "                \n",
    "                if not isinstance(embedding, list):\n",
    "                    raise ValueError(f\"Expected list embedding, got {type(embedding)}\")\n",
    "                \n",
    "                # Validate embedding structure (should be list of lists for ColBERT)\n",
    "                if embedding and isinstance(embedding[0], list):\n",
    "                    all_embeddings.append(embedding)\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid embedding structure: {type(embedding[0]) if embedding else 'empty'}\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"‚ùå API request error for text '{text[:50]}...': {e}\")\n",
    "                raise e\n",
    "            except KeyError as e:\n",
    "                logger.error(f\"‚ùå Unexpected API response structure: {e}\")\n",
    "                raise e\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error getting embedding for text '{text[:50]}...': {e}\")\n",
    "                raise e\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def get_single_embedding(self, text: str, input_type: str = \"document\") -> List[List[float]]:\n",
    "        \"\"\"Get single ColBERT embedding for a text.\"\"\"\n",
    "        embeddings = self.get_colbert_embeddings([text], input_type)\n",
    "        return embeddings[0] if embeddings else None\n",
    "\n",
    "# Initialize pipeline simulator\n",
    "pipeline_simulator = JinaColBERTEmbeddingPipeline(model_config, pipeline_config)\n",
    "\n",
    "try:\n",
    "    pipeline_simulator.initialize_tokenizer_and_model()\n",
    "    print(\"‚úÖ Jina ColBERT v2 pipeline initialized!\")\n",
    "    print(f\"üî§ Max tokens: {model_config.max_tokens}\")\n",
    "    \n",
    "    # Test embedding generation\n",
    "    test_text = \"This is a sample text for ColBERT embedding generation.\"\n",
    "    logger.info(\"üß™ Testing ColBERT embedding generation...\")\n",
    "    test_embedding = pipeline_simulator.get_single_embedding(test_text)\n",
    "    \n",
    "    if test_embedding:\n",
    "        print(f\"üßÆ ColBERT embedding shape: ({len(test_embedding)}, {len(test_embedding[0])})\")\n",
    "        print(f\"üìà Number of token vectors: {len(test_embedding)}\")\n",
    "        print(f\"üìè Vector dimension: {len(test_embedding[0])}\")\n",
    "        print(f\"üìä Sample values from first vector: {test_embedding[0][:5]}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to generate test embedding\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Error initializing pipeline: {e}\")\n",
    "    print(\"üí° Make sure your Jina API key is valid and you have internet connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39935f3",
   "metadata": {},
   "source": [
    "## 6. Process and Prepare Documents\n",
    "\n",
    "Load dan process documents dari mock JSONL data, handle text truncation, dan prepare metadata untuk indexing seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1216118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìÑ Processing documents from: /tmp/sample_corpus.jsonl\n",
      "INFO:__main__:‚úÖ Processed 10 documents\n",
      "INFO:__main__:‚úÖ Processed 10 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 10 chunks ready for ColBERT indexing\n",
      "\n",
      "üìã Sample chunk:\n",
      "ID: sample_dataset_chunk_0000\n",
      "Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "Metadata: {'chunk_id': 'sample_dataset_chunk_0000', 'title': 'Document 1', 'source': 'sample_source_1', 'category': 'technology', 'chunk_index': 0, 'word_count': 17, 'char_count': 124}\n",
      "\n",
      "üîÑ Ready for ColBERT multi-vector embedding generation...\n"
     ]
    }
   ],
   "source": [
    "def process_jsonl_documents(file_path: str, pipeline_simulator: JinaColBERTEmbeddingPipeline) -> List[Document]:\n",
    "    \"\"\"Process JSONL documents for ColBERT pipeline - same logic as original pipeline.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    logger.info(f\"üìÑ Processing documents from: {file_path}\")\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # Extract metadata (exclude 'text' field)\n",
    "                metadata = {}\n",
    "                for k, v in data.items():\n",
    "                    if k == \"text\":\n",
    "                        continue\n",
    "                    if k == \"_id\":\n",
    "                        metadata[\"chunk_id\"] = v\n",
    "                    else:\n",
    "                        metadata[k] = v\n",
    "                \n",
    "                # Get text content\n",
    "                text = data.get(\"text\", \"\")\n",
    "                \n",
    "                # Apply truncation if configured (ColBERT v2 supports up to 8192 tokens)\n",
    "                truncate_chunk_size = pipeline_simulator.pipeline_config.get(\"truncate_chunk_size\")\n",
    "                if truncate_chunk_size is not None:\n",
    "                    original_length = len(text.split())\n",
    "                    text = JinaColBERTEmbeddingPipeline.truncate_to_token_limit(\n",
    "                        text=text, \n",
    "                        tokenizer=pipeline_simulator.tokenizer, \n",
    "                        max_tokens=truncate_chunk_size\n",
    "                    )\n",
    "                    new_length = len(text.split())\n",
    "                    if original_length != new_length:\n",
    "                        logger.debug(f\"üîÑ Truncated doc {line_num}: {original_length} ‚Üí {new_length} tokens\")\n",
    "                \n",
    "                # Apply text size truncation if configured\n",
    "                truncate_text_size = pipeline_simulator.pipeline_config.get(\"truncate_text_size\")\n",
    "                if truncate_text_size is not None:\n",
    "                    text = text[:truncate_text_size] if len(text) > truncate_text_size else text\n",
    "                \n",
    "                # Create document\n",
    "                document = Document(\n",
    "                    page_content=text,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(document)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"‚ùå JSON decode error at line {line_num}: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error processing line {line_num}: {e}\")\n",
    "    \n",
    "    logger.info(f\"‚úÖ Processed {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "def filter_complex_metadata(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Filter complex metadata - simplified version.\"\"\"\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        # Simple filtering - remove any non-serializable metadata\n",
    "        filtered_metadata = {}\n",
    "        for k, v in doc.metadata.items():\n",
    "            if isinstance(v, (str, int, float, bool, type(None))):\n",
    "                filtered_metadata[k] = v\n",
    "        \n",
    "        filtered_doc = Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata=filtered_metadata\n",
    "        )\n",
    "        filtered_docs.append(filtered_doc)\n",
    "    \n",
    "    return filtered_docs\n",
    "\n",
    "def documents_to_chunks(documents: List[Document]) -> List[Chunk]:\n",
    "    \"\"\"Convert documents to chunks format.\"\"\"\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunk = Chunk(\n",
    "            content=doc.page_content,\n",
    "            id=doc.metadata[\"chunk_id\"],\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process the mock JSONL data\n",
    "documents = process_jsonl_documents(temp_jsonl_file, pipeline_simulator)\n",
    "documents = filter_complex_metadata(documents)\n",
    "chunks = documents_to_chunks(documents)\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks ready for ColBERT indexing\")\n",
    "print(f\"\\nüìã Sample chunk:\")\n",
    "print(f\"ID: {chunks[0].id}\")\n",
    "print(f\"Content: {chunks[0].content[:100]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")\n",
    "print(f\"\\nüîÑ Ready for ColBERT multi-vector embedding generation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b83ffc",
   "metadata": {},
   "source": [
    "## 7. Generate Document Embeddings\n",
    "\n",
    "Generate embeddings untuk semua document chunks menggunakan embedding model yang sudah dikonfigurasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9011134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üßÆ Generating ColBERT multi-vector embeddings for all chunks...\n",
      "Generating ColBERT embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:16<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 10 ColBERT multi-vector embeddings\n",
      "‚è±Ô∏è Embedding generation took: 16.01 seconds\n",
      "üéØ Average time per document: 1.601 seconds\n",
      "üìè ColBERT embedding structure:\n",
      "‚îú‚îÄ‚îÄ Number of token vectors: 26\n",
      "‚îú‚îÄ‚îÄ Vector dimension: 128\n",
      "‚îî‚îÄ‚îÄ Total parameters per document: 3328\n",
      "\n",
      "üìä Token count statistics across all documents:\n",
      "‚îú‚îÄ‚îÄ Min tokens: 19\n",
      "‚îú‚îÄ‚îÄ Max tokens: 29\n",
      "‚îú‚îÄ‚îÄ Average tokens: 24.4\n",
      "‚îî‚îÄ‚îÄ Total embeddings: 10\n",
      "\n",
      "üßÆ Sample values from first token vector: [0.16882324, 0.009887695, -0.17614746, -0.26831055, 0.08026123]\n",
      "‚úÖ All token vectors have consistent dimensions: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate ColBERT multi-vector embeddings for all chunks\n",
    "logger.info(\"üßÆ Generating ColBERT multi-vector embeddings for all chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract text content from chunks\n",
    "chunk_texts = [chunk.content for chunk in chunks]\n",
    "\n",
    "# Generate embeddings in smaller batches for API efficiency\n",
    "batch_size = 3  # Smaller batch size for API calls to avoid rate limits\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Generating ColBERT embeddings\"):\n",
    "    batch_texts = chunk_texts[i:i + batch_size]\n",
    "    \n",
    "    try:\n",
    "        # Get multi-vector embeddings from Jina ColBERT v2 API\n",
    "        batch_embeddings = pipeline_simulator.get_colbert_embeddings(batch_texts, input_type=\"document\")\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        # Add a small delay to respect API rate limits\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error generating embeddings for batch {i//batch_size + 1}: {e}\")\n",
    "        # For demo purposes, continue with empty embeddings\n",
    "        # In production, you would want to retry or handle this differently\n",
    "        for _ in batch_texts:\n",
    "            all_embeddings.append([])  # Empty embedding as placeholder\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Generated {len(all_embeddings)} ColBERT multi-vector embeddings\")\n",
    "print(f\"‚è±Ô∏è Embedding generation took: {embedding_time:.2f} seconds\")\n",
    "print(f\"üéØ Average time per document: {embedding_time/len(chunks):.3f} seconds\")\n",
    "\n",
    "# Validate ColBERT embeddings\n",
    "if all_embeddings and all_embeddings[0]:  # Check if we have valid embeddings\n",
    "    first_embedding = all_embeddings[0]\n",
    "    token_count = len(first_embedding)\n",
    "    vector_dim = len(first_embedding[0]) if first_embedding else 0\n",
    "    \n",
    "    print(f\"üìè ColBERT embedding structure:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Number of token vectors: {token_count}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Vector dimension: {vector_dim}\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Total parameters per document: {token_count * vector_dim}\")\n",
    "    \n",
    "    # Show stats for all embeddings\n",
    "    token_counts = [len(emb) for emb in all_embeddings if emb]\n",
    "    if token_counts:\n",
    "        print(f\"\\nüìä Token count statistics across all documents:\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Min tokens: {min(token_counts)}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Max tokens: {max(token_counts)}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Average tokens: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "        print(f\"‚îî‚îÄ‚îÄ Total embeddings: {len(token_counts)}\")\n",
    "    \n",
    "    # Sample values from first embedding\n",
    "    print(f\"\\nüßÆ Sample values from first token vector: {first_embedding[0][:5]}\")\n",
    "    \n",
    "    # Check for consistent dimensions\n",
    "    dims_consistent = all(\n",
    "        len(emb[0]) == vector_dim for emb in all_embeddings \n",
    "        if emb and len(emb) > 0\n",
    "    )\n",
    "    print(f\"‚úÖ All token vectors have consistent dimensions: {dims_consistent}\")\n",
    "else:\n",
    "    print(\"‚ùå No valid embeddings generated!\")\n",
    "    print(\"üí° Please check your Jina API key and internet connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c471d",
   "metadata": {},
   "source": [
    "## 8. Index Documents to Elasticsearch\n",
    "\n",
    "Batch index processed documents dengan embeddings ke Elasticsearch, handle existing indices dan error management seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e6c6d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìã Creating ColBERT vector store with index: colbert_sample_dataset_jina-colbert-v2\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.002s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 1 times in a row, putting on 1 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 0 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x156743ac0>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156743ac0>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.001s]\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.002s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 1 times in a row, putting on 1 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 0 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x156743ac0>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156743ac0>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.001s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 2 times in a row, putting on 2 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 1 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x156743ce0>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156743ce0>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.001s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 3 times in a row, putting on 4 second timeout\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 2 times in a row, putting on 2 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 1 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x156743ce0>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156743ce0>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.001s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 3 times in a row, putting on 4 second timeout\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 2 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x156743f00>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156743f00>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.001s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 4 times in a row, putting on 8 second timeout\n",
      "WARNING:__main__:‚ö†Ô∏è Cannot connect to Elasticsearch: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156743ac0>: Failed to establish a new connection: [Errno 61] Connection refused))\n",
      "INFO:__main__:üîÑ Using in-memory simulation mode\n",
      "INFO:__main__:üìä Existing documents in index: 0\n",
      "INFO:__main__:üìã Valid chunks with embeddings: 10/10\n",
      "INFO:__main__:üöÄ Starting batch indexing of 10 chunks with ColBERT embeddings...\n",
      "Indexing ColBERT batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 6241.52it/s]\n",
      "INFO:__main__:‚úÖ ColBERT indexing completed!\n",
      "INFO:__main__:üìä Final document count: 10\n",
      "INFO:__main__:‚è±Ô∏è Indexing took: 0.00 seconds\n",
      "INFO:__main__:üéØ Average indexing time per document: 0.000 seconds\n",
      "WARNING:elastic_transport.transport:Retrying request after failure (attempt 2 of 3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
      "    response = self.pool.urlopen(\n",
      "        method,\n",
      "    ...<4 lines>...\n",
      "        **kw,  # type: ignore[arg-type]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/retry.py\", line 449, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "    ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        enforce_content_length=enforce_content_length,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "    ~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "    ~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "        self, f\"Failed to establish a new connection: {e}\"\n",
      "    ) from e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x156743f00>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
      "    resp = node.perform_request(\n",
      "        method,\n",
      "    ...<3 lines>...\n",
      "        request_timeout=request_timeout,\n",
      "    )\n",
      "  File \"/Users/azmyaryarizaldi/Desktop/GDP/ElasticSearch/Handson/env/lib/python3.13/site-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
      "    raise err from e\n",
      "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156743f00>: Failed to establish a new connection: [Errno 61] Connection refused)\n",
      "INFO:elastic_transport.transport:GET http://localhost:9200/ [status:N/A duration:0.001s]\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 4 times in a row, putting on 8 second timeout\n",
      "WARNING:__main__:‚ö†Ô∏è Cannot connect to Elasticsearch: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x156743ac0>: Failed to establish a new connection: [Errno 61] Connection refused))\n",
      "INFO:__main__:üîÑ Using in-memory simulation mode\n",
      "INFO:__main__:üìä Existing documents in index: 0\n",
      "INFO:__main__:üìã Valid chunks with embeddings: 10/10\n",
      "INFO:__main__:üöÄ Starting batch indexing of 10 chunks with ColBERT embeddings...\n",
      "Indexing ColBERT batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 6241.52it/s]\n",
      "INFO:__main__:‚úÖ ColBERT indexing completed!\n",
      "INFO:__main__:üìä Final document count: 10\n",
      "INFO:__main__:‚è±Ô∏è Indexing took: 0.00 seconds\n",
      "INFO:__main__:üéØ Average indexing time per document: 0.000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà ColBERT Pipeline Statistics:\n",
      "‚îú‚îÄ‚îÄ Documents processed: 10\n",
      "‚îú‚îÄ‚îÄ Valid embeddings generated: 10\n",
      "‚îú‚îÄ‚îÄ Failed embeddings: 0\n",
      "‚îú‚îÄ‚îÄ Index name: colbert_sample_dataset_jina-colbert-v2\n",
      "‚îú‚îÄ‚îÄ Final document count: 10\n",
      "‚îú‚îÄ‚îÄ Vector store mode: Simulation\n",
      "‚îî‚îÄ‚îÄ Model: jina-colbert-v2 (ColBERT v2)\n",
      "\n",
      "üßÆ ColBERT Embedding Statistics:\n",
      "‚îú‚îÄ‚îÄ Total token vectors: 244\n",
      "‚îú‚îÄ‚îÄ Average vectors per document: 24.4\n",
      "‚îú‚îÄ‚îÄ Vector dimension: 128\n",
      "‚îî‚îÄ‚îÄ Total parameters: 31,232\n"
     ]
    }
   ],
   "source": [
    "# Create vector store and index ColBERT multi-vector embeddings\n",
    "dataset_name = \"sample_dataset\"\n",
    "model_for_index_name = model_config.provider_model_id.replace(\"/\", \"-\")\n",
    "index_name = f\"colbert_{dataset_name.lower()}_{model_for_index_name.lower()}\"\n",
    "\n",
    "logger.info(f\"üìã Creating ColBERT vector store with index: {index_name}\")\n",
    "\n",
    "# Initialize vector store for ColBERT embeddings\n",
    "vector_store = ElasticsearchVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding_config=model_config.model_dump()\n",
    ")\n",
    "\n",
    "# Check if index already has data (like in original pipeline)\n",
    "existing_doc_count = vector_store.count_documents()\n",
    "logger.info(f\"üìä Existing documents in index: {existing_doc_count}\")\n",
    "\n",
    "# Filter out chunks with empty embeddings\n",
    "valid_chunks = []\n",
    "valid_embeddings = []\n",
    "for chunk, embedding in zip(chunks, all_embeddings):\n",
    "    if embedding and len(embedding) > 0:  # Check if embedding is not empty\n",
    "        valid_chunks.append(chunk)\n",
    "        valid_embeddings.append(embedding)\n",
    "\n",
    "logger.info(f\"üìã Valid chunks with embeddings: {len(valid_chunks)}/{len(chunks)}\")\n",
    "\n",
    "if existing_doc_count >= len(valid_chunks):\n",
    "    logger.info(f\"‚è≠Ô∏è Index already has {existing_doc_count} documents. Skipping indexing.\")\n",
    "else:\n",
    "    logger.info(f\"üöÄ Starting batch indexing of {len(valid_chunks)} chunks with ColBERT embeddings...\")\n",
    "    \n",
    "    # Batch indexing for ColBERT multi-vector embeddings\n",
    "    batch_size = pipeline_config.get(\"batch_size\", 8)  # Smaller batch for multi-vector\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(valid_chunks), batch_size), desc=\"Indexing ColBERT batches\"):\n",
    "        batch_chunks = valid_chunks[i:i + batch_size]\n",
    "        batch_embeddings = valid_embeddings[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Add batch to vector store\n",
    "            vector_store.add_chunks_batch(batch_chunks, batch_embeddings)\n",
    "            logger.debug(f\"‚úÖ Indexed ColBERT batch {i//batch_size + 1}: {len(batch_chunks)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error indexing ColBERT batch {i//batch_size + 1}: {e}\")\n",
    "            # In real pipeline, this would raise the exception\n",
    "            # raise e\n",
    "    \n",
    "    indexing_time = time.time() - start_time\n",
    "    \n",
    "    # Verify indexing\n",
    "    final_doc_count = vector_store.count_documents()\n",
    "    logger.info(f\"‚úÖ ColBERT indexing completed!\")\n",
    "    logger.info(f\"üìä Final document count: {final_doc_count}\")\n",
    "    logger.info(f\"‚è±Ô∏è Indexing took: {indexing_time:.2f} seconds\")\n",
    "    logger.info(f\"üéØ Average indexing time per document: {indexing_time/len(valid_chunks):.3f} seconds\")\n",
    "\n",
    "# Store vector store for later use\n",
    "pipeline_simulator.vector_stores[dataset_name] = vector_store\n",
    "\n",
    "print(f\"\\nüìà ColBERT Pipeline Statistics:\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Documents processed: {len(chunks)}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Valid embeddings generated: {len(valid_embeddings)}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Failed embeddings: {len(chunks) - len(valid_embeddings)}\")  \n",
    "print(f\"‚îú‚îÄ‚îÄ Index name: {index_name}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Final document count: {vector_store.count_documents()}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Vector store mode: {'Simulation' if vector_store.use_simulation else 'Elasticsearch'}\")\n",
    "print(f\"‚îî‚îÄ‚îÄ Model: {model_config.provider_model_id} (ColBERT v2)\")\n",
    "\n",
    "# Display embedding statistics\n",
    "if valid_embeddings:\n",
    "    total_vectors = sum(len(emb) for emb in valid_embeddings)\n",
    "    avg_vectors_per_doc = total_vectors / len(valid_embeddings)\n",
    "    print(f\"\\nüßÆ ColBERT Embedding Statistics:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Total token vectors: {total_vectors:,}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Average vectors per document: {avg_vectors_per_doc:.1f}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Vector dimension: {model_config.embedding_dimensions}\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Total parameters: {total_vectors * model_config.embedding_dimensions:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66597b3e",
   "metadata": {},
   "source": [
    "## 9. Implement Semantic Search\n",
    "\n",
    "Create search functionality yang embed query text dan melakukan vector similarity search terhadap indexed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6091ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing ColBERT Semantic Search with Late Interaction:\n",
      "============================================================\n",
      "\n",
      "üîé Query 1: What is machine learning and artificial intelligence?\n",
      "üìä Query vectors: 32 tokens\n",
      "‚è±Ô∏è Search time: 0.118 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Late Interaction Score: 0.6881\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "   2. Late Interaction Score: 0.5795\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problems and recognize patt...\n",
      "      Chunk ID: sample_dataset_chunk_0002\n",
      "\n",
      "   3. Late Interaction Score: 0.5720\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "üîé Query 2: How does natural language processing work?\n",
      "üìä Query vectors: 32 tokens\n",
      "‚è±Ô∏è Search time: 0.118 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Late Interaction Score: 0.6881\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "   2. Late Interaction Score: 0.5795\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problems and recognize patt...\n",
      "      Chunk ID: sample_dataset_chunk_0002\n",
      "\n",
      "   3. Late Interaction Score: 0.5720\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "üîé Query 2: How does natural language processing work?\n",
      "üìä Query vectors: 32 tokens\n",
      "‚è±Ô∏è Search time: 0.129 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Late Interaction Score: 0.6650\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "   2. Late Interaction Score: 0.6049\n",
      "      Content: transformer models have revolutionized natural language processing with their attention mechanism an...\n",
      "      Chunk ID: sample_dataset_chunk_0006\n",
      "\n",
      "   3. Late Interaction Score: 0.5422\n",
      "      Content: retrieval - augmented generation combines information retrieval with language generation for improve...\n",
      "      Chunk ID: sample_dataset_chunk_0008\n",
      "\n",
      "üîé Query 3: Tell me about deep learning and neural networks\n",
      "üìä Query vectors: 32 tokens\n",
      "‚è±Ô∏è Search time: 0.129 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Late Interaction Score: 0.6650\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "   2. Late Interaction Score: 0.6049\n",
      "      Content: transformer models have revolutionized natural language processing with their attention mechanism an...\n",
      "      Chunk ID: sample_dataset_chunk_0006\n",
      "\n",
      "   3. Late Interaction Score: 0.5422\n",
      "      Content: retrieval - augmented generation combines information retrieval with language generation for improve...\n",
      "      Chunk ID: sample_dataset_chunk_0008\n",
      "\n",
      "üîé Query 3: Tell me about deep learning and neural networks\n",
      "üìä Query vectors: 32 tokens\n",
      "‚è±Ô∏è Search time: 0.130 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Late Interaction Score: 0.6577\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problems and recognize patt...\n",
      "      Chunk ID: sample_dataset_chunk_0002\n",
      "\n",
      "   2. Late Interaction Score: 0.5479\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "   3. Late Interaction Score: 0.5171\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "üîé Query 4: What is vector search and embeddings?\n",
      "üìä Query vectors: 32 tokens\n",
      "‚è±Ô∏è Search time: 0.130 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Late Interaction Score: 0.6577\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problems and recognize patt...\n",
      "      Chunk ID: sample_dataset_chunk_0002\n",
      "\n",
      "   2. Late Interaction Score: 0.5479\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers to learn without bein...\n",
      "      Chunk ID: sample_dataset_chunk_0000\n",
      "\n",
      "   3. Late Interaction Score: 0.5171\n",
      "      Content: natural language processing involves the interaction between computers and human language, enabling ...\n",
      "      Chunk ID: sample_dataset_chunk_0001\n",
      "\n",
      "üîé Query 4: What is vector search and embeddings?\n",
      "üìä Query vectors: 32 tokens\n",
      "‚è±Ô∏è Search time: 0.126 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Late Interaction Score: 0.6229\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabling semantic search a...\n",
      "      Chunk ID: sample_dataset_chunk_0005\n",
      "\n",
      "   2. Late Interaction Score: 0.5985\n",
      "      Content: embedding models convert text into numerical representations that capture semantic meaning and conte...\n",
      "      Chunk ID: sample_dataset_chunk_0007\n",
      "\n",
      "   3. Late Interaction Score: 0.5723\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucene for full - text se...\n",
      "      Chunk ID: sample_dataset_chunk_0004\n",
      "\n",
      "‚úÖ ColBERT semantic search testing completed!\n",
      "üéØ Late interaction scoring provides more nuanced similarity matching\n",
      "üìä Query vectors: 32 tokens\n",
      "‚è±Ô∏è Search time: 0.126 seconds\n",
      "üìä Found 10 results\n",
      "\n",
      "   1. Late Interaction Score: 0.6229\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabling semantic search a...\n",
      "      Chunk ID: sample_dataset_chunk_0005\n",
      "\n",
      "   2. Late Interaction Score: 0.5985\n",
      "      Content: embedding models convert text into numerical representations that capture semantic meaning and conte...\n",
      "      Chunk ID: sample_dataset_chunk_0007\n",
      "\n",
      "   3. Late Interaction Score: 0.5723\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucene for full - text se...\n",
      "      Chunk ID: sample_dataset_chunk_0004\n",
      "\n",
      "‚úÖ ColBERT semantic search testing completed!\n",
      "üéØ Late interaction scoring provides more nuanced similarity matching\n"
     ]
    }
   ],
   "source": [
    "class ColBERTSemanticRetriever:\n",
    "    \"\"\"ColBERT semantic retrieval class with late interaction similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: ElasticsearchVectorStore, top_k: int = 10):\n",
    "        self.vector_store = vector_store\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        magnitude1 = sum(a * a for a in vec1) ** 0.5\n",
    "        magnitude2 = sum(b * b for b in vec2) ** 0.5\n",
    "        \n",
    "        if magnitude1 == 0 or magnitude2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    def late_interaction_similarity(self, query_vectors: List[List[float]], doc_vectors: List[List[float]]) -> float:\n",
    "        \"\"\"\n",
    "        Compute ColBERT late interaction similarity.\n",
    "        For each query token, find the best matching document token, then sum.\n",
    "        \"\"\"\n",
    "        if not query_vectors or not doc_vectors:\n",
    "            return 0.0\n",
    "        \n",
    "        total_score = 0.0\n",
    "        \n",
    "        # For each query token vector\n",
    "        for q_vec in query_vectors:\n",
    "            # Find the maximum similarity with any document token vector\n",
    "            max_similarity = max(\n",
    "                self.cosine_similarity(q_vec, d_vec) \n",
    "                for d_vec in doc_vectors\n",
    "            )\n",
    "            total_score += max_similarity\n",
    "        \n",
    "        # Normalize by query length\n",
    "        return total_score / len(query_vectors)\n",
    "    \n",
    "    def search(self, query_embedding: List[List[float]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for similar documents using ColBERT late interaction.\"\"\"\n",
    "        if self.vector_store.use_simulation:\n",
    "            # Simulation mode: compute late interaction similarities in memory\n",
    "            similarities = []\n",
    "            \n",
    "            for doc in self.vector_store.simulated_docs:\n",
    "                doc_embedding = doc['embedding']\n",
    "                if not doc_embedding:  # Skip empty embeddings\n",
    "                    continue\n",
    "                    \n",
    "                similarity = self.late_interaction_similarity(query_embedding, doc_embedding)\n",
    "                similarities.append({\n",
    "                    'chunk': doc['chunk'],\n",
    "                    'score': similarity\n",
    "                })\n",
    "            \n",
    "            # Sort by similarity score (descending)\n",
    "            similarities.sort(key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            # Return top-k results\n",
    "            return similarities[:self.top_k]\n",
    "        \n",
    "        else:\n",
    "            # Real Elasticsearch mode (would need custom script for late interaction)\n",
    "            try:\n",
    "                # Note: Real Elasticsearch implementation would require a custom script\n",
    "                # for late interaction scoring. For now, we'll use a simplified approach.\n",
    "                \n",
    "                # First, get candidate documents using simple vector search\n",
    "                # This is a simplified implementation - real ColBERT would need custom scoring\n",
    "                \n",
    "                # Use the first query vector for initial retrieval\n",
    "                if not query_embedding or not query_embedding[0]:\n",
    "                    return []\n",
    "                \n",
    "                search_body = {\n",
    "                    \"query\": {\n",
    "                        \"script_score\": {\n",
    "                            \"query\": {\"match_all\": {}},\n",
    "                            \"script\": {\n",
    "                                \"source\": \"Math.max(0, cosineSimilarity(params.query_vector, 'embeddings') + 1.0)\",\n",
    "                                \"params\": {\n",
    "                                    \"query_vector\": query_embedding[0]  # Use first vector for ES search\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"size\": self.top_k,\n",
    "                    \"_source\": [\"content\", \"metadata\", \"chunk_id\", \"embeddings\"]\n",
    "                }\n",
    "                \n",
    "                response = self.vector_store.client.search(\n",
    "                    index=self.vector_store.index_name,\n",
    "                    body=search_body\n",
    "                )\n",
    "                \n",
    "                results = []\n",
    "                for hit in response['hits']['hits']:\n",
    "                    # Get document embeddings for late interaction scoring\n",
    "                    doc_embeddings = hit['_source'].get('embeddings', [])\n",
    "                    \n",
    "                    # Compute late interaction score\n",
    "                    late_interaction_score = self.late_interaction_similarity(\n",
    "                        query_embedding, doc_embeddings\n",
    "                    )\n",
    "                    \n",
    "                    chunk = Chunk(\n",
    "                        content=hit['_source']['content'],\n",
    "                        id=hit['_source']['chunk_id'],\n",
    "                        metadata=hit['_source']['metadata']\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'chunk': chunk,\n",
    "                        'score': late_interaction_score\n",
    "                    })\n",
    "                \n",
    "                # Re-sort by late interaction score\n",
    "                results.sort(key=lambda x: x['score'], reverse=True)\n",
    "                return results\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Elasticsearch ColBERT search error: {e}\")\n",
    "                return []\n",
    "\n",
    "# Initialize ColBERT semantic retriever\n",
    "retriever = ColBERTSemanticRetriever(vector_store, top_k=pipeline_config.get(\"retrieval_top_k\", 10))\n",
    "\n",
    "# Test queries for ColBERT\n",
    "test_queries = [\n",
    "    \"What is machine learning and artificial intelligence?\",\n",
    "    \"How does natural language processing work?\",\n",
    "    \"Tell me about deep learning and neural networks\",\n",
    "    \"What is vector search and embeddings?\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing ColBERT Semantic Search with Late Interaction:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüîé Query {i}: {query}\")\n",
    "    \n",
    "    try:\n",
    "        # Generate query embedding using ColBERT (query type)\n",
    "        query_embedding = pipeline_simulator.get_single_embedding(query, input_type=\"query\")\n",
    "        \n",
    "        if not query_embedding:\n",
    "            print(\"‚ùå Failed to generate query embedding\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"üìä Query vectors: {len(query_embedding)} tokens\")\n",
    "        \n",
    "        # Perform ColBERT search with late interaction\n",
    "        search_start = time.time()\n",
    "        results = retriever.search(query_embedding)\n",
    "        search_time = time.time() - search_start\n",
    "        \n",
    "        print(f\"‚è±Ô∏è Search time: {search_time:.3f} seconds\")\n",
    "        print(f\"üìä Found {len(results)} results\")\n",
    "        \n",
    "        # Display top 3 results\n",
    "        for j, result in enumerate(results[:3], 1):\n",
    "            chunk = result['chunk']\n",
    "            score = result['score']\n",
    "            print(f\"\\n   {j}. Late Interaction Score: {score:.4f}\")\n",
    "            print(f\"      Content: {chunk.content[:100]}...\")\n",
    "            print(f\"      Chunk ID: {chunk.id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing query {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úÖ ColBERT semantic search testing completed!\")\n",
    "print(\"üéØ Late interaction scoring provides more nuanced similarity matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b1fe3",
   "metadata": {},
   "source": [
    "## 10. Test Retrieval and Reranking\n",
    "\n",
    "Implement dan test document retrieval dengan optional reranking functionality, measuring retrieval accuracy dan performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "90b3cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Comprehensive ColBERT Retrieval Pipeline Test\n",
      "======================================================================\n",
      "\n",
      "üîé Test Query 1: 'machine learning artificial intelligence algorithms'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.112s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6302\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "   2. Score: 0.5850\n",
      "      Content: retrieval - augmented generation combines information retrieval with language ge...\n",
      "      Metadata: Document 9\n",
      "   3. Score: 0.5730\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "\n",
      "üîé Test Query 2: 'natural language processing text understanding'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.112s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6302\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "   2. Score: 0.5850\n",
      "      Content: retrieval - augmented generation combines information retrieval with language ge...\n",
      "      Metadata: Document 9\n",
      "   3. Score: 0.5730\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "\n",
      "üîé Test Query 2: 'natural language processing text understanding'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.125s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6488\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "   2. Score: 0.5725\n",
      "      Content: transformer models have revolutionized natural language processing with their at...\n",
      "      Metadata: Document 7\n",
      "   3. Score: 0.5440\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "\n",
      "üîé Test Query 3: 'deep learning neural networks pattern recognition'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.125s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6488\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "   2. Score: 0.5725\n",
      "      Content: transformer models have revolutionized natural language processing with their at...\n",
      "      Metadata: Document 7\n",
      "   3. Score: 0.5440\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "\n",
      "üîé Test Query 3: 'deep learning neural networks pattern recognition'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.132s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6585\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "   2. Score: 0.5458\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "   3. Score: 0.5403\n",
      "      Content: retrieval - augmented generation combines information retrieval with language ge...\n",
      "      Metadata: Document 9\n",
      "\n",
      "üîé Test Query 4: 'vector database similarity semantic search'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.132s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6585\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "   2. Score: 0.5458\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "   3. Score: 0.5403\n",
      "      Content: retrieval - augmented generation combines information retrieval with language ge...\n",
      "      Metadata: Document 9\n",
      "\n",
      "üîé Test Query 4: 'vector database similarity semantic search'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.133s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6682\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "   2. Score: 0.5718\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "   3. Score: 0.5489\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "\n",
      "üîé Test Query 5: 'elasticsearch distributed search analytics engine'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.133s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6682\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "   2. Score: 0.5718\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "   3. Score: 0.5489\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "\n",
      "üîé Test Query 5: 'elasticsearch distributed search analytics engine'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.133s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6935\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "   2. Score: 0.5485\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "   3. Score: 0.5474\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "\n",
      "üìà ColBERT Performance Summary:\n",
      "‚îú‚îÄ‚îÄ Total embedding time: 9.311s\n",
      "‚îú‚îÄ‚îÄ Average embedding time: 1.862s\n",
      "‚îú‚îÄ‚îÄ Total search time: 0.634s\n",
      "‚îú‚îÄ‚îÄ Average search time: 0.127s\n",
      "‚îî‚îÄ‚îÄ Total queries tested: 5\n",
      "\n",
      "======================================================================\n",
      "üîÑ Testing ColBERT with Reranking Enabled\n",
      "üî¨ Comprehensive ColBERT Retrieval Pipeline Test\n",
      "======================================================================\n",
      "\n",
      "üîé Test Query 1: 'machine learning artificial intelligence algorithms'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.133s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.6935\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "   2. Score: 0.5485\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "   3. Score: 0.5474\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "\n",
      "üìà ColBERT Performance Summary:\n",
      "‚îú‚îÄ‚îÄ Total embedding time: 9.311s\n",
      "‚îú‚îÄ‚îÄ Average embedding time: 1.862s\n",
      "‚îú‚îÄ‚îÄ Total search time: 0.634s\n",
      "‚îú‚îÄ‚îÄ Average search time: 0.127s\n",
      "‚îî‚îÄ‚îÄ Total queries tested: 5\n",
      "\n",
      "======================================================================\n",
      "üîÑ Testing ColBERT with Reranking Enabled\n",
      "üî¨ Comprehensive ColBERT Retrieval Pipeline Test\n",
      "======================================================================\n",
      "\n",
      "üîé Test Query 1: 'machine learning artificial intelligence algorithms'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.333s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.8120\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "      Overlap: 0.800, Length: 0.707, Diversity: 1.000\n",
      "   2. Score: 0.5120\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "      Overlap: 0.200, Length: 0.707, Diversity: 1.000\n",
      "   3. Score: 0.4113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.000, Length: 0.704, Diversity: 1.000\n",
      "\n",
      "üîé Test Query 2: 'natural language processing text understanding'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.333s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.8120\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "      Overlap: 0.800, Length: 0.707, Diversity: 1.000\n",
      "   2. Score: 0.5120\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "      Overlap: 0.200, Length: 0.707, Diversity: 1.000\n",
      "   3. Score: 0.4113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.000, Length: 0.704, Diversity: 1.000\n",
      "\n",
      "üîé Test Query 2: 'natural language processing text understanding'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.120s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.7113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.600, Length: 0.704, Diversity: 1.000\n",
      "   2. Score: 0.7098\n",
      "      Content: transformer models have revolutionized natural language processing with their at...\n",
      "      Metadata: Document 7\n",
      "      Overlap: 0.600, Length: 0.699, Diversity: 1.000\n",
      "   3. Score: 0.5091\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "      Overlap: 0.200, Length: 0.697, Diversity: 1.000\n",
      "\n",
      "üîé Test Query 3: 'deep learning neural networks pattern recognition'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.120s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.7113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.600, Length: 0.704, Diversity: 1.000\n",
      "   2. Score: 0.7098\n",
      "      Content: transformer models have revolutionized natural language processing with their at...\n",
      "      Metadata: Document 7\n",
      "      Overlap: 0.600, Length: 0.699, Diversity: 1.000\n",
      "   3. Score: 0.5091\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "      Overlap: 0.200, Length: 0.697, Diversity: 1.000\n",
      "\n",
      "üîé Test Query 3: 'deep learning neural networks pattern recognition'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.106s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.7453\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "      Overlap: 0.667, Length: 0.707, Diversity: 1.000\n",
      "   2. Score: 0.4953\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "      Overlap: 0.167, Length: 0.707, Diversity: 1.000\n",
      "   3. Score: 0.4113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.000, Length: 0.704, Diversity: 1.000\n",
      "\n",
      "üîé Test Query 4: 'vector database similarity semantic search'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.106s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.7453\n",
      "      Content: deep learning uses neural networks with multiple layers to solve complex problem...\n",
      "      Metadata: Document 3\n",
      "      Overlap: 0.667, Length: 0.707, Diversity: 1.000\n",
      "   2. Score: 0.4953\n",
      "      Content: machine learning is a subset of artificial intelligence that enables computers t...\n",
      "      Metadata: Document 1\n",
      "      Overlap: 0.167, Length: 0.707, Diversity: 1.000\n",
      "   3. Score: 0.4113\n",
      "      Content: natural language processing involves the interaction between computers and human...\n",
      "      Metadata: Document 2\n",
      "      Overlap: 0.000, Length: 0.704, Diversity: 1.000\n",
      "\n",
      "üîé Test Query 4: 'vector database similarity semantic search'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.122s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.7863\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "      Overlap: 0.800, Length: 0.704, Diversity: 0.875\n",
      "   2. Score: 0.5972\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "      Overlap: 0.400, Length: 0.702, Diversity: 0.933\n",
      "   3. Score: 0.5091\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "      Overlap: 0.200, Length: 0.697, Diversity: 1.000\n",
      "\n",
      "üîé Test Query 5: 'elasticsearch distributed search analytics engine'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.122s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.7863\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "      Overlap: 0.800, Length: 0.704, Diversity: 0.875\n",
      "   2. Score: 0.5972\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "      Overlap: 0.400, Length: 0.702, Diversity: 0.933\n",
      "   3. Score: 0.5091\n",
      "      Content: embedding models convert text into numerical representations that capture semant...\n",
      "      Metadata: Document 8\n",
      "      Overlap: 0.200, Length: 0.697, Diversity: 1.000\n",
      "\n",
      "üîé Test Query 5: 'elasticsearch distributed search analytics engine'\n",
      "--------------------------------------------------\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.098s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.9017\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "      Overlap: 1.000, Length: 0.709, Diversity: 0.944\n",
      "   2. Score: 0.4972\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "      Overlap: 0.200, Length: 0.702, Diversity: 0.933\n",
      "   3. Score: 0.4863\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "      Overlap: 0.200, Length: 0.704, Diversity: 0.875\n",
      "\n",
      "üìà ColBERT Performance Summary:\n",
      "‚îú‚îÄ‚îÄ Total embedding time: 7.858s\n",
      "‚îú‚îÄ‚îÄ Average embedding time: 1.572s\n",
      "‚îú‚îÄ‚îÄ Total search time: 0.779s\n",
      "‚îú‚îÄ‚îÄ Average search time: 0.156s\n",
      "‚îú‚îÄ‚îÄ Total rerank time: 0.000s\n",
      "‚îú‚îÄ‚îÄ Average rerank time: 0.000s\n",
      "‚îî‚îÄ‚îÄ Total queries tested: 5\n",
      "\n",
      "üéØ ColBERT vs Traditional Embeddings:\n",
      "‚îú‚îÄ‚îÄ ‚úÖ Token-level interactions for better semantic matching\n",
      "‚îú‚îÄ‚îÄ ‚úÖ Handles long documents more effectively\n",
      "‚îú‚îÄ‚îÄ ‚úÖ Late interaction preserves fine-grained relevance\n",
      "‚îú‚îÄ‚îÄ ‚ö†Ô∏è  Higher computational cost during search\n",
      "‚îú‚îÄ‚îÄ ‚ö†Ô∏è  More complex indexing and storage requirements\n",
      "‚îî‚îÄ‚îÄ üìä Better accuracy-efficiency trade-off for semantic search\n",
      "üßÆ Query embedding: 32 token vectors\n",
      "üìä Initial retrieval: 10 results in 0.098s\n",
      "üîÑ ColBERT reranking completed in 0.000s\n",
      "\n",
      "üéØ Top 3 Final Results:\n",
      "   1. Score: 0.9017\n",
      "      Content: elasticsearch is a distributed search and analytics engine built on apache lucen...\n",
      "      Metadata: Document 5\n",
      "      Overlap: 1.000, Length: 0.709, Diversity: 0.944\n",
      "   2. Score: 0.4972\n",
      "      Content: semantic search goes beyond keyword matching to understand the meaning and inten...\n",
      "      Metadata: Document 10\n",
      "      Overlap: 0.200, Length: 0.702, Diversity: 0.933\n",
      "   3. Score: 0.4863\n",
      "      Content: vector databases store and search high - dimensional vectors efficiently, enabli...\n",
      "      Metadata: Document 6\n",
      "      Overlap: 0.200, Length: 0.704, Diversity: 0.875\n",
      "\n",
      "üìà ColBERT Performance Summary:\n",
      "‚îú‚îÄ‚îÄ Total embedding time: 7.858s\n",
      "‚îú‚îÄ‚îÄ Average embedding time: 1.572s\n",
      "‚îú‚îÄ‚îÄ Total search time: 0.779s\n",
      "‚îú‚îÄ‚îÄ Average search time: 0.156s\n",
      "‚îú‚îÄ‚îÄ Total rerank time: 0.000s\n",
      "‚îú‚îÄ‚îÄ Average rerank time: 0.000s\n",
      "‚îî‚îÄ‚îÄ Total queries tested: 5\n",
      "\n",
      "üéØ ColBERT vs Traditional Embeddings:\n",
      "‚îú‚îÄ‚îÄ ‚úÖ Token-level interactions for better semantic matching\n",
      "‚îú‚îÄ‚îÄ ‚úÖ Handles long documents more effectively\n",
      "‚îú‚îÄ‚îÄ ‚úÖ Late interaction preserves fine-grained relevance\n",
      "‚îú‚îÄ‚îÄ ‚ö†Ô∏è  Higher computational cost during search\n",
      "‚îú‚îÄ‚îÄ ‚ö†Ô∏è  More complex indexing and storage requirements\n",
      "‚îî‚îÄ‚îÄ üìä Better accuracy-efficiency trade-off for semantic search\n"
     ]
    }
   ],
   "source": [
    "class ColBERTReranker:\n",
    "    \"\"\"ColBERT-aware reranker that considers token-level interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rerank(self, query: str, chunks: List[Chunk], query_embedding: List[List[float]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Rerank using ColBERT late interaction + text-based features.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Text-based scoring features\n",
    "            query_words = set(query.lower().split())\n",
    "            chunk_words = set(chunk.content.lower().split())\n",
    "            \n",
    "            # Keyword overlap score\n",
    "            overlap_score = len(query_words.intersection(chunk_words)) / len(query_words) if query_words else 0\n",
    "            \n",
    "            # Length preference (prefer moderate length chunks)\n",
    "            length_score = 1.0 / (1.0 + abs(len(chunk.content.split()) - 100) * 0.005)\n",
    "            \n",
    "            # Diversity score (prefer chunks with varied vocabulary)\n",
    "            diversity_score = len(chunk_words) / len(chunk.content.split()) if chunk.content.split() else 0\n",
    "            \n",
    "            # Combined rerank score\n",
    "            rerank_score = overlap_score * 0.5 + length_score * 0.3 + diversity_score * 0.2\n",
    "            \n",
    "            results.append({\n",
    "                'chunk': chunk,\n",
    "                'score': rerank_score,\n",
    "                'overlap_score': overlap_score,\n",
    "                'length_score': length_score,\n",
    "                'diversity_score': diversity_score\n",
    "            })\n",
    "        \n",
    "        # Sort by rerank score\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return results\n",
    "\n",
    "def comprehensive_colbert_retrieval_test():\n",
    "    \"\"\"Comprehensive test of the ColBERT retrieval pipeline.\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Comprehensive ColBERT Retrieval Pipeline Test\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test configuration\n",
    "    test_queries = [\n",
    "        \"machine learning artificial intelligence algorithms\",\n",
    "        \"natural language processing text understanding\", \n",
    "        \"deep learning neural networks pattern recognition\",\n",
    "        \"vector database similarity semantic search\",\n",
    "        \"elasticsearch distributed search analytics engine\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize ColBERT reranker\n",
    "    reranker = ColBERTReranker()\n",
    "    \n",
    "    # Performance metrics\n",
    "    total_embedding_time = 0\n",
    "    total_search_time = 0\n",
    "    total_rerank_time = 0\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîé Test Query {i}: '{query}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Generate query embedding (ColBERT multi-vector)\n",
    "            embed_start = time.time()\n",
    "            query_embedding = pipeline_simulator.get_single_embedding(query, input_type=\"query\")\n",
    "            embed_time = time.time() - embed_start\n",
    "            total_embedding_time += embed_time\n",
    "            \n",
    "            if not query_embedding:\n",
    "                print(\"‚ùå Failed to generate query embedding\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"üßÆ Query embedding: {len(query_embedding)} token vectors\")\n",
    "            \n",
    "            # Step 2: Initial ColBERT retrieval with late interaction\n",
    "            search_start = time.time()\n",
    "            initial_results = retriever.search(query_embedding)\n",
    "            search_time = time.time() - search_start\n",
    "            total_search_time += search_time\n",
    "            \n",
    "            print(f\"üìä Initial retrieval: {len(initial_results)} results in {search_time:.3f}s\")\n",
    "            \n",
    "            # Step 3: Reranking (if enabled in config)\n",
    "            if pipeline_config.get(\"use_reranker\", False):\n",
    "                rerank_start = time.time()\n",
    "                \n",
    "                # Extract chunks for reranking\n",
    "                initial_chunks = [result['chunk'] for result in initial_results]\n",
    "                reranked_results = reranker.rerank(query, initial_chunks, query_embedding)\n",
    "                \n",
    "                rerank_time = time.time() - rerank_start\n",
    "                total_rerank_time += rerank_time\n",
    "                \n",
    "                print(f\"üîÑ ColBERT reranking completed in {rerank_time:.3f}s\")\n",
    "                final_results = reranked_results\n",
    "            else:\n",
    "                final_results = initial_results\n",
    "            \n",
    "            # Step 4: Display results\n",
    "            print(f\"\\nüéØ Top 3 Final Results:\")\n",
    "            for j, result in enumerate(final_results[:3], 1):\n",
    "                chunk = result['chunk']\n",
    "                score = result['score']\n",
    "                \n",
    "                print(f\"   {j}. Score: {score:.4f}\")\n",
    "                print(f\"      Content: {chunk.content[:80]}...\")\n",
    "                print(f\"      Metadata: {chunk.metadata.get('title', 'N/A')}\")\n",
    "                \n",
    "                # Show reranking details if available\n",
    "                if 'overlap_score' in result:\n",
    "                    print(f\"      Overlap: {result['overlap_score']:.3f}, Length: {result['length_score']:.3f}, Diversity: {result['diversity_score']:.3f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing query {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nüìà ColBERT Performance Summary:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Total embedding time: {total_embedding_time:.3f}s\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Average embedding time: {total_embedding_time/len(test_queries):.3f}s\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Total search time: {total_search_time:.3f}s\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Average search time: {total_search_time/len(test_queries):.3f}s\")\n",
    "    if total_rerank_time > 0:\n",
    "        print(f\"‚îú‚îÄ‚îÄ Total rerank time: {total_rerank_time:.3f}s\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Average rerank time: {total_rerank_time/len(test_queries):.3f}s\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Total queries tested: {len(test_queries)}\")\n",
    "\n",
    "# Run comprehensive ColBERT test\n",
    "comprehensive_colbert_retrieval_test()\n",
    "\n",
    "# Test with reranking enabled\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ Testing ColBERT with Reranking Enabled\")\n",
    "pipeline_config[\"use_reranker\"] = True\n",
    "comprehensive_colbert_retrieval_test()\n",
    "pipeline_config[\"use_reranker\"] = False  # Reset\n",
    "\n",
    "print(f\"\\nüéØ ColBERT vs Traditional Embeddings:\")\n",
    "print(\"‚îú‚îÄ‚îÄ ‚úÖ Token-level interactions for better semantic matching\")\n",
    "print(\"‚îú‚îÄ‚îÄ ‚úÖ Handles long documents more effectively\") \n",
    "print(\"‚îú‚îÄ‚îÄ ‚úÖ Late interaction preserves fine-grained relevance\")\n",
    "print(\"‚îú‚îÄ‚îÄ ‚ö†Ô∏è  Higher computational cost during search\")\n",
    "print(\"‚îú‚îÄ‚îÄ ‚ö†Ô∏è  More complex indexing and storage requirements\")\n",
    "print(\"‚îî‚îÄ‚îÄ üìä Better accuracy-efficiency trade-off for semantic search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b893f7",
   "metadata": {},
   "source": [
    "## 11. Cleanup and Performance Monitoring\n",
    "\n",
    "Monitor index performance, document counts, dan implement cleanup procedures untuk removing test indices seperti pada pipeline asli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "918e3374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ColBERT Performance Monitoring & Index Statistics\n",
      "======================================================================\n",
      "\n",
      "üìã Dataset: sample_dataset\n",
      "‚îú‚îÄ‚îÄ Index name: colbert_sample_dataset_jina-colbert-v2\n",
      "‚îú‚îÄ‚îÄ Document count: 10\n",
      "‚îú‚îÄ‚îÄ Storage mode: Simulation\n",
      "‚îú‚îÄ‚îÄ Embedding model: jina-colbert-v2\n",
      "‚îú‚îÄ‚îÄ Vector dimensions: 128\n",
      "‚îî‚îÄ‚îÄ Multi-vector architecture: ColBERT late interaction\n",
      "\n",
      "======================================================================\n",
      "üéØ FINAL COLBERT PIPELINE SUMMARY\n",
      "\n",
      "üìà ColBERT Pipeline Execution Summary\n",
      "======================================================================\n",
      "üèóÔ∏è ColBERT Pipeline Configuration:\n",
      "‚îú‚îÄ‚îÄ Model: jina-colbert-v2\n",
      "‚îú‚îÄ‚îÄ Provider: jinaai\n",
      "‚îú‚îÄ‚îÄ API endpoint: https://api.jina.ai/v1/multi-vector\n",
      "‚îú‚îÄ‚îÄ Vector dimensions: 128\n",
      "‚îú‚îÄ‚îÄ Max tokens: 8192\n",
      "‚îú‚îÄ‚îÄ Batch size: 16\n",
      "‚îú‚îÄ‚îÄ Top-K retrieval: 10\n",
      "‚îî‚îÄ‚îÄ Reranking enabled: False\n",
      "\n",
      "üìä Data Processing Results:\n",
      "‚îú‚îÄ‚îÄ Documents processed: 10\n",
      "‚îú‚îÄ‚îÄ Valid embeddings generated: 10\n",
      "‚îú‚îÄ‚îÄ Failed embeddings: 0\n",
      "‚îú‚îÄ‚îÄ Index name: colbert_sample_dataset_jina-colbert-v2\n",
      "‚îú‚îÄ‚îÄ Storage mode: Simulation\n",
      "‚îî‚îÄ‚îÄ Architecture: ColBERT multi-vector late interaction\n",
      "\n",
      "üßÆ ColBERT Embedding Statistics:\n",
      "‚îú‚îÄ‚îÄ Total token vectors: 244\n",
      "‚îú‚îÄ‚îÄ Average vectors per document: 24.4\n",
      "‚îú‚îÄ‚îÄ Vector dimension: 128\n",
      "‚îú‚îÄ‚îÄ Total parameters: 31,232\n",
      "‚îî‚îÄ‚îÄ Storage efficiency: 0.02x vs traditional\n",
      "\n",
      "‚úÖ ColBERT pipeline completed successfully!\n",
      "üéØ This simulation demonstrates the complete ColBERT embedding pipeline\n",
      "üìù Key improvements over traditional embeddings:\n",
      "   ‚îú‚îÄ‚îÄ Token-level semantic matching\n",
      "   ‚îú‚îÄ‚îÄ Late interaction scoring\n",
      "   ‚îú‚îÄ‚îÄ Better handling of long documents\n",
      "   ‚îî‚îÄ‚îÄ More nuanced relevance assessment\n",
      "\n",
      "üèÅ ColBERT Elasticsearch Embedding Pipeline Simulation Complete!\n",
      "üìÖ Completed at: 2025-07-24 09:18:18.557805\n",
      "üîó This notebook successfully simulates the entire ColBERT pipeline using Jina AI API\n",
      "üöÄ ColBERT v2 provides superior semantic search through late interaction architecture\n"
     ]
    }
   ],
   "source": [
    "def performance_monitoring():\n",
    "    \"\"\"Monitor performance and index statistics for ColBERT implementation.\"\"\"\n",
    "    \n",
    "    print(\"üìä ColBERT Performance Monitoring & Index Statistics\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Vector store statistics\n",
    "    for dataset_name, vs in pipeline_simulator.vector_stores.items():\n",
    "        print(f\"\\nüìã Dataset: {dataset_name}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Index name: {vs.index_name}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Document count: {vs.count_documents()}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Storage mode: {'Simulation' if vs.use_simulation else 'Elasticsearch'}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Embedding model: {vs.embedding_config.get('provider_model_id', 'Unknown')}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Vector dimensions: {vs.embedding_config.get('embedding_dimensions', 'Unknown')}\")\n",
    "        print(f\"‚îî‚îÄ‚îÄ Multi-vector architecture: ColBERT late interaction\")\n",
    "        \n",
    "        if not vs.use_simulation and hasattr(vs, 'client'):\n",
    "            try:\n",
    "                # Get index stats from Elasticsearch\n",
    "                stats = vs.client.indices.stats(index=vs.index_name)\n",
    "                index_stats = stats['indices'][vs.index_name]['total']\n",
    "                \n",
    "                print(f\"‚îú‚îÄ‚îÄ Index size: {index_stats['store']['size_in_bytes']} bytes\")\n",
    "                print(f\"‚îú‚îÄ‚îÄ Documents indexed: {index_stats['docs']['count']}\")\n",
    "                print(f\"‚îî‚îÄ‚îÄ Search operations: {index_stats['search']['query_total']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚îú‚îÄ‚îÄ ‚ö†Ô∏è Cannot retrieve ES stats: {e}\")\n",
    "\n",
    "def cleanup_pipeline():\n",
    "    \"\"\"Cleanup vector stores and temporary files.\"\"\"\n",
    "    \n",
    "    print(\"\\nüßπ ColBERT Pipeline Cleanup Procedures\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    try:\n",
    "        if os.path.exists(temp_jsonl_file):\n",
    "            os.remove(temp_jsonl_file)\n",
    "            print(f\"‚úÖ Removed temporary file: {temp_jsonl_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error removing temp file: {e}\")\n",
    "    \n",
    "    # Cleanup vector stores (similar to original pipeline)\n",
    "    if pipeline_simulator.vector_stores:\n",
    "        print(f\"üóëÔ∏è Cleaning up {len(pipeline_simulator.vector_stores)} ColBERT vector stores...\")\n",
    "        \n",
    "        for dataset_name, vs in pipeline_simulator.vector_stores.items():\n",
    "            try:\n",
    "                if not vs.use_simulation:\n",
    "                    # In real implementation, this would delete the index\n",
    "                    # vs.client.indices.delete(index=vs.index_name)\n",
    "                    print(f\"üóëÔ∏è Would delete ColBERT index: {vs.index_name}\")\n",
    "                else:\n",
    "                    vs.simulated_docs.clear()\n",
    "                    print(f\"üóëÔ∏è Cleared simulated ColBERT data for: {dataset_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error cleaning up {dataset_name}: {e}\")\n",
    "        \n",
    "        # Clear the vector stores dictionary\n",
    "        pipeline_simulator.vector_stores = {}\n",
    "        print(\"‚úÖ ColBERT vector stores cleanup completed\")\n",
    "\n",
    "def pipeline_summary():\n",
    "    \"\"\"Generate final ColBERT pipeline summary.\"\"\"\n",
    "    \n",
    "    print(\"\\nüìà ColBERT Pipeline Execution Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"üèóÔ∏è ColBERT Pipeline Configuration:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Model: {model_config.provider_model_id}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Provider: {model_config.provider}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ API endpoint: {model_config.api_endpoint}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Vector dimensions: {model_config.embedding_dimensions}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Max tokens: {model_config.max_tokens}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Batch size: {pipeline_config.get('batch_size', 16)}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Top-K retrieval: {pipeline_config.get('retrieval_top_k', 10)}\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Reranking enabled: {pipeline_config.get('use_reranker', False)}\")\n",
    "    \n",
    "    print(f\"\\nüìä Data Processing Results:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Documents processed: {len(chunks)}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Valid embeddings generated: {len([e for e in all_embeddings if e])}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Failed embeddings: {len([e for e in all_embeddings if not e])}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Index name: {index_name}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Storage mode: {'Simulation' if vector_store.use_simulation else 'Elasticsearch'}\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Architecture: ColBERT multi-vector late interaction\")\n",
    "    \n",
    "    # ColBERT specific statistics\n",
    "    if valid_embeddings:\n",
    "        total_vectors = sum(len(emb) for emb in valid_embeddings)\n",
    "        avg_vectors_per_doc = total_vectors / len(valid_embeddings)\n",
    "        total_params = total_vectors * model_config.embedding_dimensions\n",
    "        \n",
    "        print(f\"\\nüßÆ ColBERT Embedding Statistics:\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Total token vectors: {total_vectors:,}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Average vectors per document: {avg_vectors_per_doc:.1f}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Vector dimension: {model_config.embedding_dimensions}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Total parameters: {total_params:,}\")\n",
    "        print(f\"‚îî‚îÄ‚îÄ Storage efficiency: {total_params/(len(valid_embeddings)*512*384):.2f}x vs traditional\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ ColBERT pipeline completed successfully!\")\n",
    "    print(f\"üéØ This simulation demonstrates the complete ColBERT embedding pipeline\")\n",
    "    print(f\"üìù Key improvements over traditional embeddings:\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ Token-level semantic matching\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ Late interaction scoring\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ Better handling of long documents\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ More nuanced relevance assessment\")\n",
    "\n",
    "# Run monitoring and summary\n",
    "performance_monitoring()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ FINAL COLBERT PIPELINE SUMMARY\")\n",
    "pipeline_summary()\n",
    "\n",
    "# Optional: Run cleanup (uncomment if you want to clean up)\n",
    "# cleanup_pipeline()\n",
    "\n",
    "print(f\"\\nüèÅ ColBERT Elasticsearch Embedding Pipeline Simulation Complete!\")\n",
    "print(f\"üìÖ Completed at: {datetime.now()}\")\n",
    "print(\"üîó This notebook successfully simulates the entire ColBERT pipeline using Jina AI API\")\n",
    "print(\"üöÄ ColBERT v2 provides superior semantic search through late interaction architecture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
